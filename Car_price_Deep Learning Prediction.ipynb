{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Loading all the necessary libraries for the Deep learning model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, add\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11914, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the car data stored as an iage into the Python notebook\n",
    "car_data = pd.read_csv(\"data.csv\")\n",
    "car_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8084, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have chosen only the car details for the rows that have non empty cells\n",
    "car_data = car_data.dropna()\n",
    "car_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the dummies for all the categorical string variable to build a numberical database\n",
    "dummy1 = pd.get_dummies(car_data[\"Make\"], prefix='Make')\n",
    "dummy2 = pd.get_dummies(car_data[\"Model\"], prefix='Model')\n",
    "dummy3 = pd.get_dummies(car_data[\"Engine Fuel Type\"], prefix='EFT')\n",
    "dummy4 = pd.get_dummies(car_data[\"Transmission Type\"], prefix='TNT')\n",
    "dummy5 = pd.get_dummies(car_data[\"Driven_Wheels\"], prefix='DRW')\n",
    "dummy6 = pd.get_dummies(car_data[\"Market Category\"], prefix='MTC')\n",
    "dummy7 = pd.get_dummies(car_data[\"Vehicle Size\"], prefix='VSZ')\n",
    "dummy8 = pd.get_dummies(car_data[\"Vehicle Style\"], prefix='VST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After creating the dummies, we concatenate all the dataframe into consolidated dataframe\n",
    "car_dummy = pd.concat((dummy1, dummy2, dummy3, dummy4, dummy5, dummy6, dummy7, dummy8), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we concatenate the main dataframe, with the consolidated dummy variable dataframe\n",
    "car_data_final = pd.concat((car_data, car_dummy), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping redundant columns creating bias to the dataset created\n",
    "car_data_f = car_data_final.drop(columns={'Make_Oldsmobile', 'Model_RS 6', 'EFT_electric', 'TNT_UNKNOWN', 'DRW_four wheel drive',\n",
    "                                          'MTC_Exotic,Luxury,High-Performance,Hybrid', 'VSZ_Large', 'VST_Cargo Minivan',\n",
    "                                          'Make', 'Model', 'Engine Fuel Type', 'Transmission Type', 'Driven_Wheels',\n",
    "                                          'Market Category', 'Vehicle Size', 'Vehicle Style'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8084, 866)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final shape of the cars price prediction dataset\n",
    "car_data_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we move the target column to the end of the dataframe\n",
    "cols = list(car_data_f.columns.values)\n",
    "cols.pop(cols.index('MSRP'))\n",
    "car_data_exe = car_data_f[cols+['MSRP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have analysed all the correlation between the different columns created. \n",
    "corp = car_data_exe.iloc[:,0:865].corr()\n",
    "\n",
    "#There is no to be worrried positive or negative correlation between any two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5416, 865) (2668, 865) (5416, 1) (2668, 1)\n"
     ]
    }
   ],
   "source": [
    "#Creating a numpy array for Neural Network configuration\n",
    "car_dat = np.array(car_data_exe)\n",
    "X = car_dat[:,0:865]\n",
    "y = car_dat[:,865:866]\n",
    "\n",
    "#We have not scaled the dataset as the direct implcation of verious important parameters are not captured when scaled down.\n",
    "#Hence we use the dataset with its original values. \n",
    "\n",
    "#Splitting the independent and the dependent variable into the training and the test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Shape of the train and the test array\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Fully connected Neural network for training the model into 7 different hidden layers of 2^n nodes. We use the most\n",
    "#common activation method \"\"ReLu\" to have the maximum training of the NN over the model. As we are working on the preiction\n",
    "#model we finally use 1 node final output layer with the activation of \"linear\" mode.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=865, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#We then compile the model with the loss factor of multiple linear regression being the \"MSE\" and we use one of the most\n",
    "#effective linear model optimizer \"rmsprop\"\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4332 samples, validate on 1084 samples\n",
      "Epoch 1/500\n",
      "4332/4332 [==============================] - 1s 226us/step - loss: 7928222876.3657 - val_loss: 3909876368.0590\n",
      "Epoch 2/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 6247564040.5097 - val_loss: 3265988799.0554\n",
      "Epoch 3/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 5847257973.9538 - val_loss: 2997522029.8155\n",
      "Epoch 4/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 5608801866.2235 - val_loss: 2825104431.9410\n",
      "Epoch 5/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 5409946505.3370 - val_loss: 2642993719.2620\n",
      "Epoch 6/500\n",
      "4332/4332 [==============================] - 0s 71us/step - loss: 5103242193.4331 - val_loss: 2357289237.7269\n",
      "Epoch 7/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 4677788636.2179 - val_loss: 1986477401.2694\n",
      "Epoch 8/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 4224772758.2789 - val_loss: 1658087047.4391\n",
      "Epoch 9/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 3824181052.9861 - val_loss: 1451692881.5941\n",
      "Epoch 10/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 3497341663.9705 - val_loss: 1289271197.0480\n",
      "Epoch 11/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 3291074817.8910 - val_loss: 1201141723.9852\n",
      "Epoch 12/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 3156044661.0083 - val_loss: 1153451457.8893\n",
      "Epoch 13/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 2998359886.5669 - val_loss: 1107596217.0332\n",
      "Epoch 14/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 2889408766.1681 - val_loss: 1057642930.4207\n",
      "Epoch 15/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 2770426034.7036 - val_loss: 1031179408.4133\n",
      "Epoch 16/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 2677544684.3804 - val_loss: 1033186643.1882\n",
      "Epoch 17/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 2591194453.9834 - val_loss: 1082150643.8376\n",
      "Epoch 18/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 2499521575.4164 - val_loss: 1016430276.6052\n",
      "Epoch 19/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 2474342206.0203 - val_loss: 1175395257.1513\n",
      "Epoch 20/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 2367968139.2872 - val_loss: 1024966369.9483\n",
      "Epoch 21/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 2382385655.8449 - val_loss: 1086552903.7343\n",
      "Epoch 22/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 2310192246.1607 - val_loss: 1052194105.9779\n",
      "Epoch 23/500\n",
      "4332/4332 [==============================] - 0s 65us/step - loss: 2255745582.3601 - val_loss: 1503594667.4539\n",
      "Epoch 24/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 2228035485.2521 - val_loss: 1163910566.1402\n",
      "Epoch 25/500\n",
      "4332/4332 [==============================] - 0s 65us/step - loss: 2156811722.6076 - val_loss: 993278756.6052\n",
      "Epoch 26/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 2142555529.4552 - val_loss: 1688737687.6162\n",
      "Epoch 27/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 2092250958.5669 - val_loss: 1029733529.2694\n",
      "Epoch 28/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 2066135891.5753 - val_loss: 1101003410.0664\n",
      "Epoch 29/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1987304871.5642 - val_loss: 1180588438.5535\n",
      "Epoch 30/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1956656199.1210 - val_loss: 1006186416.1181\n",
      "Epoch 31/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1954797201.2853 - val_loss: 1005448032.8266\n",
      "Epoch 32/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1905649435.6270 - val_loss: 992483899.0406\n",
      "Epoch 33/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1892733092.8458 - val_loss: 1087206628.4280\n",
      "Epoch 34/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1886784606.9806 - val_loss: 972640368.0590\n",
      "Epoch 35/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1820620013.5328 - val_loss: 985490161.7712\n",
      "Epoch 36/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1789607438.2419 - val_loss: 1010806738.0074\n",
      "Epoch 37/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1703542750.4783 - val_loss: 1009173707.6310\n",
      "Epoch 38/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1733479858.2308 - val_loss: 971795331.3653\n",
      "Epoch 39/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1731180952.0517 - val_loss: 953315420.7528\n",
      "Epoch 40/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1636456182.8994 - val_loss: 943091025.9483\n",
      "Epoch 41/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1651060972.2918 - val_loss: 981467985.7712\n",
      "Epoch 42/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1659521883.3315 - val_loss: 906532635.5129\n",
      "Epoch 43/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1514625830.8403 - val_loss: 925557640.6790\n",
      "Epoch 44/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1615328240.7387 - val_loss: 926396895.8819\n",
      "Epoch 45/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 1517387082.9326 - val_loss: 914494659.7196\n",
      "Epoch 46/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1553256235.2576 - val_loss: 960384395.0406\n",
      "Epoch 47/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1601182373.7765 - val_loss: 965215889.9483\n",
      "Epoch 48/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 1529769343.3500 - val_loss: 892427859.3653\n",
      "Epoch 49/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1527479071.6898 - val_loss: 916293192.1476\n",
      "Epoch 50/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1514567467.7452 - val_loss: 886670576.1181\n",
      "Epoch 51/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1490271053.1782 - val_loss: 911132868.4280\n",
      "Epoch 52/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1503672976.2216 - val_loss: 891967131.9262\n",
      "Epoch 53/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1489901665.9206 - val_loss: 971356377.6827\n",
      "Epoch 54/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1499983369.8984 - val_loss: 946757331.7196\n",
      "Epoch 55/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1446783826.6002 - val_loss: 921328042.9815\n",
      "Epoch 56/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1475527619.0139 - val_loss: 858285259.1587\n",
      "Epoch 57/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1468595416.9529 - val_loss: 869257353.0332\n",
      "Epoch 58/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1432163264.8717 - val_loss: 929208651.9262\n",
      "Epoch 59/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1450469525.0822 - val_loss: 942200307.3653\n",
      "Epoch 60/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1457924936.1551 - val_loss: 895215931.4539\n",
      "Epoch 61/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1428101860.5946 - val_loss: 838575318.5535\n",
      "Epoch 62/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 1428936832.2068 - val_loss: 961994737.6531\n",
      "Epoch 63/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1401800771.5900 - val_loss: 920020200.6199\n",
      "Epoch 64/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1459378072.9825 - val_loss: 884897951.3506\n",
      "Epoch 65/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1416993254.6777 - val_loss: 881286286.2878\n",
      "Epoch 66/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1395134190.0646 - val_loss: 821787568.8266\n",
      "Epoch 67/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1421942215.7858 - val_loss: 891136104.9742\n",
      "Epoch 68/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1413765327.6307 - val_loss: 881997878.7306\n",
      "Epoch 69/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1346581467.6270 - val_loss: 796334996.9004\n",
      "Epoch 70/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 1334140882.3047 - val_loss: 891201219.0701\n",
      "Epoch 71/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1347410723.4275 - val_loss: 890482785.5351\n",
      "Epoch 72/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1376527178.5633 - val_loss: 1503480103.9114\n",
      "Epoch 73/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1409110404.8606 - val_loss: 818712280.2066\n",
      "Epoch 74/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1357912811.1542 - val_loss: 913893679.8229\n",
      "Epoch 75/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1385829421.7987 - val_loss: 775067731.5424\n",
      "Epoch 76/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1380187841.4478 - val_loss: 802115628.2804\n",
      "Epoch 77/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 1386766319.0545 - val_loss: 848847797.7269\n",
      "Epoch 78/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1370301816.9825 - val_loss: 786606780.9889\n",
      "Epoch 79/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1330981775.7193 - val_loss: 842446950.4354\n",
      "Epoch 80/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1377566826.1644 - val_loss: 863369210.7454\n",
      "Epoch 81/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1354129248.9751 - val_loss: 858915310.4059\n",
      "Epoch 82/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1338450288.9898 - val_loss: 1503492423.6753\n",
      "Epoch 83/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1386190132.4469 - val_loss: 810392422.0812\n",
      "Epoch 84/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 1338912481.8837 - val_loss: 734994966.7306\n",
      "Epoch 85/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1319078557.8283 - val_loss: 783424636.9889\n",
      "Epoch 86/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1324829345.8615 - val_loss: 1001263844.9594\n",
      "Epoch 87/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1310410453.9834 - val_loss: 741943634.3616\n",
      "Epoch 88/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1315575359.0545 - val_loss: 826366059.2768\n",
      "Epoch 89/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1307064405.0822 - val_loss: 764518866.8339\n",
      "Epoch 90/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1320310791.9778 - val_loss: 798775706.8635\n",
      "Epoch 91/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1322543254.0277 - val_loss: 730688519.9114\n",
      "Epoch 92/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 1299594729.5143 - val_loss: 748048108.8708\n",
      "Epoch 93/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1292778422.2050 - val_loss: 794009407.2325\n",
      "Epoch 94/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1305263966.3306 - val_loss: 703703808.1181\n",
      "Epoch 95/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1318578585.6473 - val_loss: 724515943.0849\n",
      "Epoch 96/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1269795353.8541 - val_loss: 844793005.8745\n",
      "Epoch 97/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1316377262.1681 - val_loss: 702804770.5387\n",
      "Epoch 98/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1267913581.3407 - val_loss: 800625643.5129\n",
      "Epoch 99/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1300794950.3970 - val_loss: 695608221.9336\n",
      "Epoch 100/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1262741669.4811 - val_loss: 699108659.2472\n",
      "Epoch 101/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 1268064946.9252 - val_loss: 707163512.1476\n",
      "Epoch 102/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1258281511.2392 - val_loss: 671430859.1587\n",
      "Epoch 103/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1294132452.6685 - val_loss: 673209511.3210\n",
      "Epoch 104/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 1222878080.3398 - val_loss: 684896852.4871\n",
      "Epoch 105/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1242919663.2909 - val_loss: 768696063.4686\n",
      "Epoch 106/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1295666008.2881 - val_loss: 675181495.6753\n",
      "Epoch 107/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1241508650.3121 - val_loss: 780910453.1365\n",
      "Epoch 108/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1229854774.1163 - val_loss: 687876433.4170\n",
      "Epoch 109/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1240744051.8412 - val_loss: 670451483.8081\n",
      "Epoch 110/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1265434690.9548 - val_loss: 659891172.1328\n",
      "Epoch 111/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1254021612.9861 - val_loss: 683391645.9926\n",
      "Epoch 112/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1224526380.9935 - val_loss: 674767282.3026\n",
      "Epoch 113/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1230969794.6814 - val_loss: 683745317.0185\n",
      "Epoch 114/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1199362398.7738 - val_loss: 670793077.5498\n",
      "Epoch 115/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1223023722.5042 - val_loss: 655106983.4982\n",
      "Epoch 116/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1201796464.1034 - val_loss: 1528657114.0959\n",
      "Epoch 117/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1239491791.7784 - val_loss: 695471222.5535\n",
      "Epoch 118/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1199794115.0729 - val_loss: 686722343.3801\n",
      "Epoch 119/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1203131043.9446 - val_loss: 641055344.8856\n",
      "Epoch 120/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1173772346.7258 - val_loss: 702673780.1328\n",
      "Epoch 121/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1213564306.6297 - val_loss: 630280235.9262\n",
      "Epoch 122/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1161230749.8283 - val_loss: 665345129.4465\n",
      "Epoch 123/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1190929149.9760 - val_loss: 626887702.6716\n",
      "Epoch 124/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 1166859647.0102 - val_loss: 648250789.9041\n",
      "Epoch 125/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1208774447.7488 - val_loss: 666278942.4649\n",
      "Epoch 126/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1188321818.0314 - val_loss: 783154356.1328\n",
      "Epoch 127/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1131027841.3444 - val_loss: 768603821.6974\n",
      "Epoch 128/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1187674127.3500 - val_loss: 713395298.7159\n",
      "Epoch 129/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1154839387.0065 - val_loss: 627691915.2177\n",
      "Epoch 130/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1199770193.5217 - val_loss: 622380303.4096\n",
      "Epoch 131/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1161326680.9381 - val_loss: 631573362.1255\n",
      "Epoch 132/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1159238792.1108 - val_loss: 639323978.4502\n",
      "Epoch 133/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1172033682.8070 - val_loss: 800402599.3210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1103479218.8218 - val_loss: 830576572.3395\n",
      "Epoch 135/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1130742635.5014 - val_loss: 713393044.6642\n",
      "Epoch 136/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1134187224.7904 - val_loss: 592790345.2694\n",
      "Epoch 137/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1132550394.1939 - val_loss: 612221256.7970\n",
      "Epoch 138/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1113739754.4451 - val_loss: 592904539.7491\n",
      "Epoch 139/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1140967542.2936 - val_loss: 784259794.7749\n",
      "Epoch 140/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1153708003.7673 - val_loss: 598274428.2214\n",
      "Epoch 141/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1128597587.7673 - val_loss: 614634118.0221\n",
      "Epoch 142/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1121699273.1154 - val_loss: 645533778.8339\n",
      "Epoch 143/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 1150849771.2872 - val_loss: 590268788.2509\n",
      "Epoch 144/500\n",
      "4332/4332 [==============================] - 0s 66us/step - loss: 1066932589.9760 - val_loss: 651828907.6310\n",
      "Epoch 145/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1083869893.2004 - val_loss: 592721506.4797\n",
      "Epoch 146/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 1121966453.1634 - val_loss: 598261053.5793\n",
      "Epoch 147/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1077795302.5743 - val_loss: 620976308.2509\n",
      "Epoch 148/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1098152805.8135 - val_loss: 568492218.6863\n",
      "Epoch 149/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1126483674.2382 - val_loss: 570101039.8229\n",
      "Epoch 150/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1103978513.2853 - val_loss: 630891785.2694\n",
      "Epoch 151/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1113572802.1126 - val_loss: 577355112.3247\n",
      "Epoch 152/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1104740865.0046 - val_loss: 568914844.5166\n",
      "Epoch 153/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 1035004459.4718 - val_loss: 607446794.5683\n",
      "Epoch 154/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1115440707.7525 - val_loss: 565627446.4945\n",
      "Epoch 155/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 1050507436.3066 - val_loss: 639098369.8303\n",
      "Epoch 156/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1145729520.0591 - val_loss: 591202350.8782\n",
      "Epoch 157/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1071077204.2253 - val_loss: 714528827.0406\n",
      "Epoch 158/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1084580067.7525 - val_loss: 598420885.2546\n",
      "Epoch 159/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 1043574415.2909 - val_loss: 594373963.9852\n",
      "Epoch 160/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1049165820.5429 - val_loss: 554180735.3506\n",
      "Epoch 161/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1050429888.2068 - val_loss: 583007762.9520\n",
      "Epoch 162/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1089398909.9612 - val_loss: 546910924.0443\n",
      "Epoch 163/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1025804412.7941 - val_loss: 548360621.2841\n",
      "Epoch 164/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 1075674932.4838 - val_loss: 530595569.6531\n",
      "Epoch 165/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1020346739.6048 - val_loss: 603821791.7048\n",
      "Epoch 166/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1076190632.9972 - val_loss: 572308853.6679\n",
      "Epoch 167/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1053704441.0416 - val_loss: 588790063.2915\n",
      "Epoch 168/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1073101177.5217 - val_loss: 548662952.6790\n",
      "Epoch 169/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1036464854.4709 - val_loss: 669734933.7860\n",
      "Epoch 170/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1051935594.2235 - val_loss: 544547975.6162\n",
      "Epoch 171/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 998217660.2770 - val_loss: 664077849.9779\n",
      "Epoch 172/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 1023295132.7202 - val_loss: 527807628.8708\n",
      "Epoch 173/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 1020543641.7064 - val_loss: 527020617.0332\n",
      "Epoch 174/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1014575878.8846 - val_loss: 525286024.8561\n",
      "Epoch 175/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 998191724.5577 - val_loss: 536206918.2583\n",
      "Epoch 176/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 982830633.0637 - val_loss: 551372676.2509\n",
      "Epoch 177/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1009370379.7156 - val_loss: 519324329.4465\n",
      "Epoch 178/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 950622000.0813 - val_loss: 521723061.6089\n",
      "Epoch 179/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1003845813.1782 - val_loss: 552392974.2878\n",
      "Epoch 180/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 1019433380.7128 - val_loss: 520206235.5129\n",
      "Epoch 181/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1017859256.0739 - val_loss: 511861884.2804\n",
      "Epoch 182/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 1010708786.0388 - val_loss: 624663955.3653\n",
      "Epoch 183/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 959098756.7645 - val_loss: 518710801.6531\n",
      "Epoch 184/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 1013304289.4848 - val_loss: 570607427.1292\n",
      "Epoch 185/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 941000419.9889 - val_loss: 500272711.0258\n",
      "Epoch 186/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 1013645194.8661 - val_loss: 494965340.5166\n",
      "Epoch 187/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 984163264.9972 - val_loss: 602804834.8930\n",
      "Epoch 188/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 981852211.2946 - val_loss: 494952067.1292\n",
      "Epoch 189/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 976064323.5309 - val_loss: 528300412.2214\n",
      "Epoch 190/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 944309449.3666 - val_loss: 800177095.8524\n",
      "Epoch 191/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 969492018.8440 - val_loss: 490019148.3985\n",
      "Epoch 192/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 967044067.7747 - val_loss: 462225540.9594\n",
      "Epoch 193/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 986692313.2041 - val_loss: 494696998.2583\n",
      "Epoch 194/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 916113933.6731 - val_loss: 490033107.3063\n",
      "Epoch 195/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 989201422.8550 - val_loss: 645292398.9963\n",
      "Epoch 196/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 962612975.5199 - val_loss: 509994875.6605\n",
      "Epoch 197/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 955987410.1127 - val_loss: 469223770.4502\n",
      "Epoch 198/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 942011888.6796 - val_loss: 709625451.8672\n",
      "Epoch 199/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 912166419.6934 - val_loss: 743068967.9114\n",
      "Epoch 200/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 936123406.0572 - val_loss: 436425503.3210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 962438678.1976 - val_loss: 427609066.6273\n",
      "Epoch 202/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 943066117.7322 - val_loss: 455067258.8044\n",
      "Epoch 203/500\n",
      "4332/4332 [==============================] - 0s 65us/step - loss: 957613110.9806 - val_loss: 470597864.7970\n",
      "Epoch 204/500\n",
      "4332/4332 [==============================] - 0s 65us/step - loss: 928145026.6150 - val_loss: 442525884.3395\n",
      "Epoch 205/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 907548255.6898 - val_loss: 440126435.2177\n",
      "Epoch 206/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 928050252.6057 - val_loss: 479961337.7122\n",
      "Epoch 207/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 853832008.0000 - val_loss: 448716960.6494\n",
      "Epoch 208/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 922602001.6620 - val_loss: 442679219.1882\n",
      "Epoch 209/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 871370314.1348 - val_loss: 417556813.9336\n",
      "Epoch 210/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 961452331.0803 - val_loss: 476404922.6863\n",
      "Epoch 211/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 887733447.3204 - val_loss: 1126927765.4908\n",
      "Epoch 212/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 919280438.3638 - val_loss: 417831793.5941\n",
      "Epoch 213/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 906221251.9668 - val_loss: 939795245.6974\n",
      "Epoch 214/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 925642970.2456 - val_loss: 410423810.8635\n",
      "Epoch 215/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 897999840.2770 - val_loss: 433332638.2878\n",
      "Epoch 216/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 922866741.8283 - val_loss: 415251386.0959\n",
      "Epoch 217/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 909624176.9455 - val_loss: 405603140.1033\n",
      "Epoch 218/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 892063855.5272 - val_loss: 445009718.3469\n",
      "Epoch 219/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 852702757.5623 - val_loss: 397655767.4982\n",
      "Epoch 220/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 836357232.2327 - val_loss: 558828112.4723\n",
      "Epoch 221/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 922848035.2428 - val_loss: 423523356.5166\n",
      "Epoch 222/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 853683072.7608 - val_loss: 410094256.4133\n",
      "Epoch 223/500\n",
      "4332/4332 [==============================] - 0s 67us/step - loss: 844762744.8864 - val_loss: 816585068.1328\n",
      "Epoch 224/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 852081856.7054 - val_loss: 399117194.0959\n",
      "Epoch 225/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 889950793.1080 - val_loss: 376828283.0996\n",
      "Epoch 226/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 878799041.9871 - val_loss: 383632017.1218\n",
      "Epoch 227/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 885023743.1062 - val_loss: 418779129.5646\n",
      "Epoch 228/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 872836914.7479 - val_loss: 384931583.2325\n",
      "Epoch 229/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 830887361.7507 - val_loss: 387539347.2768\n",
      "Epoch 230/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 883887702.3084 - val_loss: 465210699.8081\n",
      "Epoch 231/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 848715516.9825 - val_loss: 2119705108.9004\n",
      "Epoch 232/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 837494238.6851 - val_loss: 1336906087.4391\n",
      "Epoch 233/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 912607711.4977 - val_loss: 368539420.9889\n",
      "Epoch 234/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 772349710.2419 - val_loss: 368738350.0517\n",
      "Epoch 235/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 844493524.0628 - val_loss: 367920867.3063\n",
      "Epoch 236/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 840243665.9575 - val_loss: 564700114.0664\n",
      "Epoch 237/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 860523340.6094 - val_loss: 367061519.1439\n",
      "Epoch 238/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 826718129.2927 - val_loss: 385876103.3210\n",
      "Epoch 239/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 877756776.0776 - val_loss: 375593703.2325\n",
      "Epoch 240/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 823028234.0979 - val_loss: 340821006.3173\n",
      "Epoch 241/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 837102058.0018 - val_loss: 374593285.8155\n",
      "Epoch 242/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 848574874.1274 - val_loss: 350236307.5129\n",
      "Epoch 243/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 923009665.1893 - val_loss: 385370847.4982\n",
      "Epoch 244/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 783446829.0452 - val_loss: 860877247.5277\n",
      "Epoch 245/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 805632385.5660 - val_loss: 381070171.0406\n",
      "Epoch 246/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 813039246.8107 - val_loss: 337204565.8450\n",
      "Epoch 247/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 823631199.1874 - val_loss: 476261531.0406\n",
      "Epoch 248/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 851664953.3813 - val_loss: 317043646.1993\n",
      "Epoch 249/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 847651306.3084 - val_loss: 315052810.0959\n",
      "Epoch 250/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 828262522.0240 - val_loss: 324919608.4428\n",
      "Epoch 251/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 840960503.2872 - val_loss: 333657060.7823\n",
      "Epoch 252/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 801024759.0286 - val_loss: 316884914.8339\n",
      "Epoch 253/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 847250506.1274 - val_loss: 312457151.1734\n",
      "Epoch 254/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 835788782.4561 - val_loss: 415964424.7970\n",
      "Epoch 255/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 726744195.3610 - val_loss: 391199477.4317\n",
      "Epoch 256/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 810197514.9621 - val_loss: 379989833.4465\n",
      "Epoch 257/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 794476760.5134 - val_loss: 442773014.0221\n",
      "Epoch 258/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 790521586.8735 - val_loss: 347638647.9114\n",
      "Epoch 259/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 774370923.3093 - val_loss: 1835588124.2214\n",
      "Epoch 260/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 839078825.2151 - val_loss: 338321285.7565\n",
      "Epoch 261/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 792122264.7682 - val_loss: 337285638.1402\n",
      "Epoch 262/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 809929146.3195 - val_loss: 343469387.8376\n",
      "Epoch 263/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 762612193.9982 - val_loss: 1997868878.1697\n",
      "Epoch 264/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 784183602.6593 - val_loss: 318409305.6531\n",
      "Epoch 265/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 807988660.9935 - val_loss: 313431870.0369\n",
      "Epoch 266/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 779544190.6962 - val_loss: 362882149.8155\n",
      "Epoch 267/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 801271096.5429 - val_loss: 357487737.3875\n",
      "Epoch 268/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4332/4332 [==============================] - 0s 59us/step - loss: 769582095.0102 - val_loss: 314940565.4908\n",
      "Epoch 269/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 711612662.8957 - val_loss: 320261710.4059\n",
      "Epoch 270/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 798931353.2336 - val_loss: 371270206.7306\n",
      "Epoch 271/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 811719701.4515 - val_loss: 370966370.1107\n",
      "Epoch 272/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 755454063.6380 - val_loss: 317557178.0664\n",
      "Epoch 273/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 734118486.3490 - val_loss: 377974623.4982\n",
      "Epoch 274/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 770103419.1283 - val_loss: 322250930.0369\n",
      "Epoch 275/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 801896014.4044 - val_loss: 294304072.4723\n",
      "Epoch 276/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 751888801.9871 - val_loss: 377609771.7048\n",
      "Epoch 277/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 797746712.8717 - val_loss: 335746001.5055\n",
      "Epoch 278/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 690547347.4090 - val_loss: 386830659.0406\n",
      "Epoch 279/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 796239277.6547 - val_loss: 306321474.7011\n",
      "Epoch 280/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 654717393.4441 - val_loss: 357840113.2694\n",
      "Epoch 281/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 815037657.0452 - val_loss: 299411498.8930\n",
      "Epoch 282/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 752143913.4404 - val_loss: 351638087.8672\n",
      "Epoch 283/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 770455943.9261 - val_loss: 301884151.7343\n",
      "Epoch 284/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 734710983.3204 - val_loss: 409082139.8081\n",
      "Epoch 285/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 760120309.2114 - val_loss: 405523725.7860\n",
      "Epoch 286/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 767174393.6399 - val_loss: 756277117.2841\n",
      "Epoch 287/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 746527247.5531 - val_loss: 515837629.1070\n",
      "Epoch 288/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 697057116.4543 - val_loss: 287449046.5240\n",
      "Epoch 289/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 792514097.5956 - val_loss: 294800066.9520\n",
      "Epoch 290/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 723756126.5078 - val_loss: 379244302.9077\n",
      "Epoch 291/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 729266255.1505 - val_loss: 278095737.3284\n",
      "Epoch 292/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 701367328.1699 - val_loss: 294417220.8856\n",
      "Epoch 293/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 686797844.6870 - val_loss: 378747345.0923\n",
      "Epoch 294/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 809621520.7239 - val_loss: 531049039.5867\n",
      "Epoch 295/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 745456740.5799 - val_loss: 396104732.7823\n",
      "Epoch 296/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 642435731.4571 - val_loss: 265974925.5203\n",
      "Epoch 297/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 755389755.1099 - val_loss: 300966424.5314\n",
      "Epoch 298/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 758032020.8163 - val_loss: 264828855.6015\n",
      "Epoch 299/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 716520281.7470 - val_loss: 283208411.5867\n",
      "Epoch 300/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 744211851.7304 - val_loss: 275517894.4207\n",
      "Epoch 301/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 771268940.6648 - val_loss: 278156866.9668\n",
      "Epoch 302/500\n",
      "4332/4332 [==============================] - 0s 66us/step - loss: 725258793.1376 - val_loss: 297098203.8967\n",
      "Epoch 303/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 765473974.9067 - val_loss: 317471046.6863\n",
      "Epoch 304/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 671915834.9880 - val_loss: 317206115.6015\n",
      "Epoch 305/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 762801693.1265 - val_loss: 331812408.5609\n",
      "Epoch 306/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 721996975.9298 - val_loss: 339785776.9889\n",
      "Epoch 307/500\n",
      "4332/4332 [==============================] - 0s 53us/step - loss: 733277506.1053 - val_loss: 1099754212.2509\n",
      "Epoch 308/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 718462936.3804 - val_loss: 295571455.5572\n",
      "Epoch 309/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 666017432.5725 - val_loss: 1012858576.2362\n",
      "Epoch 310/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 738929611.3130 - val_loss: 268552009.7860\n",
      "Epoch 311/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 731947807.2946 - val_loss: 341877499.3948\n",
      "Epoch 312/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 651578121.0489 - val_loss: 315691289.4760\n",
      "Epoch 313/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 665712264.6205 - val_loss: 282439057.9926\n",
      "Epoch 314/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 725775021.7396 - val_loss: 342895893.9926\n",
      "Epoch 315/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 693587415.3869 - val_loss: 263214315.4244\n",
      "Epoch 316/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 664356827.2281 - val_loss: 915993945.9779\n",
      "Epoch 317/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 677853733.7876 - val_loss: 263039925.2989\n",
      "Epoch 318/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 705611721.1708 - val_loss: 281092242.3173\n",
      "Epoch 319/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 697529601.9206 - val_loss: 332536796.4133\n",
      "Epoch 320/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 653547063.6934 - val_loss: 656263583.8819\n",
      "Epoch 321/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 725971346.7627 - val_loss: 259275654.8044\n",
      "Epoch 322/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 627775508.9030 - val_loss: 425394872.5609\n",
      "Epoch 323/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 672903428.3029 - val_loss: 340708739.5424\n",
      "Epoch 324/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 732949966.5596 - val_loss: 276280225.4613\n",
      "Epoch 325/500\n",
      "4332/4332 [==============================] - ETA: 0s - loss: 716936799.707 - 0s 59us/step - loss: 694959229.1006 - val_loss: 271704252.1624\n",
      "Epoch 326/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 635195643.4903 - val_loss: 261241833.7417\n",
      "Epoch 327/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 700194430.5374 - val_loss: 249261201.6974\n",
      "Epoch 328/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 728973247.5937 - val_loss: 251954975.1144\n",
      "Epoch 329/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 610371812.3804 - val_loss: 328278226.4502\n",
      "Epoch 330/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 635290540.4617 - val_loss: 314922097.7417\n",
      "Epoch 331/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 626485493.9575 - val_loss: 317086406.4207\n",
      "Epoch 332/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 641816273.8837 - val_loss: 255107152.7528\n",
      "Epoch 333/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 677142435.2059 - val_loss: 809709052.7528\n",
      "Epoch 334/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 661763343.4645 - val_loss: 257151130.0221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 648256177.2410 - val_loss: 372080084.8561\n",
      "Epoch 336/500\n",
      "4332/4332 [==============================] - 0s 66us/step - loss: 671266950.9954 - val_loss: 272328258.1402\n",
      "Epoch 337/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 657578321.0157 - val_loss: 252014630.4945\n",
      "Epoch 338/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 691092523.5088 - val_loss: 282401364.8266\n",
      "Epoch 339/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 632238124.4026 - val_loss: 257907517.7417\n",
      "Epoch 340/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 670077610.6039 - val_loss: 403648657.2989\n",
      "Epoch 341/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 637932782.0794 - val_loss: 241228842.0664\n",
      "Epoch 342/500\n",
      "4332/4332 [==============================] - 0s 68us/step - loss: 687242882.9991 - val_loss: 538891220.3690\n",
      "Epoch 343/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 650716260.0222 - val_loss: 263073054.0221\n",
      "Epoch 344/500\n",
      "4332/4332 [==============================] - 0s 56us/step - loss: 704488290.3786 - val_loss: 385821281.7712\n",
      "Epoch 345/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 584985755.9187 - val_loss: 282751974.3616\n",
      "Epoch 346/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 626468159.9077 - val_loss: 257668446.6273\n",
      "Epoch 347/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 687358727.4718 - val_loss: 319552751.6162\n",
      "Epoch 348/500\n",
      "4332/4332 [==============================] - ETA: 0s - loss: 700926647.176 - 0s 54us/step - loss: 649527143.1099 - val_loss: 298257193.5941\n",
      "Epoch 349/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 699602455.7378 - val_loss: 259913965.3137\n",
      "Epoch 350/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 706787667.7452 - val_loss: 322745040.1919\n",
      "Epoch 351/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 651827937.0877 - val_loss: 324636716.2952\n",
      "Epoch 352/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 691356550.2789 - val_loss: 277534106.8782\n",
      "Epoch 353/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 609167836.3287 - val_loss: 443684036.7823\n",
      "Epoch 354/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 642317987.9926 - val_loss: 334646503.9410\n",
      "Epoch 355/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 647562454.2752 - val_loss: 240707558.5092\n",
      "Epoch 356/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 620319495.0656 - val_loss: 276955129.6974\n",
      "Epoch 357/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 584814315.7673 - val_loss: 1080837077.0185\n",
      "Epoch 358/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 653497905.6140 - val_loss: 535928352.2362\n",
      "Epoch 359/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 690874163.6694 - val_loss: 290799591.2030\n",
      "Epoch 360/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 635253478.5226 - val_loss: 308614212.0443\n",
      "Epoch 361/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 704464294.0129 - val_loss: 321079904.2362\n",
      "Epoch 362/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 584423265.8246 - val_loss: 267593601.2103\n",
      "Epoch 363/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 664160023.3980 - val_loss: 399729214.4354\n",
      "Epoch 364/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 605581567.3093 - val_loss: 586903685.6974\n",
      "Epoch 365/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 606513148.4986 - val_loss: 307193750.3764\n",
      "Epoch 366/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 665587579.0822 - val_loss: 291046040.4428\n",
      "Epoch 367/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 663201945.3112 - val_loss: 1126417370.3911\n",
      "Epoch 368/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 641836700.7498 - val_loss: 240993323.6458\n",
      "Epoch 369/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 540928619.5383 - val_loss: 266627734.2288\n",
      "Epoch 370/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 608820705.1911 - val_loss: 342346691.8967\n",
      "Epoch 371/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 622109072.8052 - val_loss: 272332306.3173\n",
      "Epoch 372/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 597970949.1745 - val_loss: 278474292.3690\n",
      "Epoch 373/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 570695730.1274 - val_loss: 258154111.2325\n",
      "Epoch 374/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 684343541.1819 - val_loss: 242246885.4613\n",
      "Epoch 375/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 685693686.3343 - val_loss: 246594563.5277\n",
      "Epoch 376/500\n",
      "4332/4332 [==============================] - 0s 65us/step - loss: 532296340.7091 - val_loss: 303709732.5018\n",
      "Epoch 377/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 591819134.6297 - val_loss: 255732240.2657\n",
      "Epoch 378/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 653683258.5799 - val_loss: 251273662.2288\n",
      "Epoch 379/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 713177590.0462 - val_loss: 264247995.8819\n",
      "Epoch 380/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 619221516.5910 - val_loss: 256204732.7970\n",
      "Epoch 381/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 652634576.5688 - val_loss: 253318656.7675\n",
      "Epoch 382/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 647432227.1468 - val_loss: 291261830.9077\n",
      "Epoch 383/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 643965124.9972 - val_loss: 295212823.7638\n",
      "Epoch 384/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 633441041.1893 - val_loss: 246932274.5978\n",
      "Epoch 385/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 613500934.2161 - val_loss: 361086994.3911\n",
      "Epoch 386/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 552813557.8596 - val_loss: 763613022.1107\n",
      "Epoch 387/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 663916047.8818 - val_loss: 271874348.3690\n",
      "Epoch 388/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 582137877.6602 - val_loss: 267813796.0295\n",
      "Epoch 389/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 581567384.8255 - val_loss: 598925424.8856\n",
      "Epoch 390/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 573898542.8809 - val_loss: 1716568257.0627\n",
      "Epoch 391/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 643877724.8089 - val_loss: 290869092.8561\n",
      "Epoch 392/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 512862133.3629 - val_loss: 346057523.1882\n",
      "Epoch 393/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 635205204.9160 - val_loss: 245836277.7269\n",
      "Epoch 394/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 523222171.6491 - val_loss: 337832464.0738\n",
      "Epoch 395/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 654441526.5743 - val_loss: 1369687920.1771\n",
      "Epoch 396/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 601934520.8864 - val_loss: 281872091.2030\n",
      "Epoch 397/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 575293242.3675 - val_loss: 292720988.9742\n",
      "Epoch 398/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 638852976.8199 - val_loss: 712568411.4539\n",
      "Epoch 399/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 601319841.7932 - val_loss: 230972849.7860\n",
      "Epoch 400/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 595763710.7036 - val_loss: 357060265.4465\n",
      "Epoch 401/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 593789908.5355 - val_loss: 286696589.1808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 635055149.5291 - val_loss: 376604169.7417\n",
      "Epoch 403/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 509445953.0748 - val_loss: 236544181.6089\n",
      "Epoch 404/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 549716062.2050 - val_loss: 644881491.2472\n",
      "Epoch 405/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 536716226.9695 - val_loss: 240692186.2731\n",
      "Epoch 406/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 524481687.8633 - val_loss: 233168741.4760\n",
      "Epoch 407/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 576455428.7387 - val_loss: 237416633.1070\n",
      "Epoch 408/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 590135968.2142 - val_loss: 228447938.0664\n",
      "Epoch 409/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 588507124.3583 - val_loss: 299188189.8007\n",
      "Epoch 410/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 610572087.5697 - val_loss: 258109577.1365\n",
      "Epoch 411/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 523205972.7830 - val_loss: 258990065.1661\n",
      "Epoch 412/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 590678089.9612 - val_loss: 232726304.6790\n",
      "Epoch 413/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 568773558.8181 - val_loss: 230017583.2915\n",
      "Epoch 414/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 573776733.3666 - val_loss: 277389619.2177\n",
      "Epoch 415/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 561586182.9584 - val_loss: 243776655.6310\n",
      "Epoch 416/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 513046274.7258 - val_loss: 226582790.8635\n",
      "Epoch 417/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 582701695.6196 - val_loss: 232934864.8561\n",
      "Epoch 418/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 552014346.5337 - val_loss: 260565123.0996\n",
      "Epoch 419/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 621094906.8957 - val_loss: 285259018.3469\n",
      "Epoch 420/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 556459501.8837 - val_loss: 272873811.4244\n",
      "Epoch 421/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 520212830.5965 - val_loss: 335255306.5978\n",
      "Epoch 422/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 546059729.5106 - val_loss: 275073711.6162\n",
      "Epoch 423/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 531242466.9511 - val_loss: 258576829.8450\n",
      "Epoch 424/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 572001776.9492 - val_loss: 228573455.5720\n",
      "Epoch 425/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 482946888.5836 - val_loss: 260716218.4207\n",
      "Epoch 426/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 629460780.3989 - val_loss: 246731568.2509\n",
      "Epoch 427/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 534119946.7368 - val_loss: 214505948.3247\n",
      "Epoch 428/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 532066869.1893 - val_loss: 215426914.2435\n",
      "Epoch 429/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 572148502.2253 - val_loss: 304528487.8229\n",
      "Epoch 430/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 535967901.4811 - val_loss: 221854924.6790\n",
      "Epoch 431/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 552805540.6611 - val_loss: 275725912.7085\n",
      "Epoch 432/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 611164200.5762 - val_loss: 232797173.7269\n",
      "Epoch 433/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 677288162.7627 - val_loss: 228713662.8339\n",
      "Epoch 434/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 517408070.2419 - val_loss: 258608844.7675\n",
      "Epoch 435/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 555191832.6685 - val_loss: 337581427.0849\n",
      "Epoch 436/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 546244904.2622 - val_loss: 283555201.8007\n",
      "Epoch 437/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 559833407.0766 - val_loss: 218539120.6052\n",
      "Epoch 438/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 485367983.8966 - val_loss: 279094990.2288\n",
      "Epoch 439/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 508692239.2133 - val_loss: 252312416.7085\n",
      "Epoch 440/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 554008144.1884 - val_loss: 2049952261.7860\n",
      "Epoch 441/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 590219660.3361 - val_loss: 222625002.7011\n",
      "Epoch 442/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 594670583.4497 - val_loss: 532079111.9114\n",
      "Epoch 443/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 498294050.2918 - val_loss: 340345931.3653\n",
      "Epoch 444/500\n",
      "4332/4332 [==============================] - 0s 55us/step - loss: 550542026.4894 - val_loss: 239432963.3653\n",
      "Epoch 445/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 571456710.0129 - val_loss: 230734467.9114\n",
      "Epoch 446/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 529224299.7821 - val_loss: 245051831.6458\n",
      "Epoch 447/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 565850864.9843 - val_loss: 571115978.7454\n",
      "Epoch 448/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 554642319.9280 - val_loss: 244450269.7565\n",
      "Epoch 449/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 492071295.0434 - val_loss: 341338661.6089\n",
      "Epoch 450/500\n",
      "4332/4332 [==============================] - 0s 64us/step - loss: 594207604.8273 - val_loss: 244389281.4908\n",
      "Epoch 451/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 576339149.2484 - val_loss: 255526173.1513\n",
      "Epoch 452/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 510904669.7987 - val_loss: 287537802.5830\n",
      "Epoch 453/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 465773474.7258 - val_loss: 648541514.8635\n",
      "Epoch 454/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 524002340.1293 - val_loss: 296391905.8598\n",
      "Epoch 455/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 474616848.2401 - val_loss: 208398157.2399\n",
      "Epoch 456/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 584994963.4164 - val_loss: 696899558.9668\n",
      "Epoch 457/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 519719739.8006 - val_loss: 209171965.7712\n",
      "Epoch 458/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 542739421.4515 - val_loss: 274807832.8856\n",
      "Epoch 459/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 510321515.8356 - val_loss: 260418144.5166\n",
      "Epoch 460/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 573481158.5115 - val_loss: 367562783.0258\n",
      "Epoch 461/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 507963958.5706 - val_loss: 270828973.1513\n",
      "Epoch 462/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 518533193.8966 - val_loss: 206956574.6716\n",
      "Epoch 463/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 460199257.8283 - val_loss: 245881089.6531\n",
      "Epoch 464/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 439384027.3149 - val_loss: 217664171.4834\n",
      "Epoch 465/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 579943461.1376 - val_loss: 315086650.1255\n",
      "Epoch 466/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 550699970.4672 - val_loss: 221686135.3210\n",
      "Epoch 467/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 478162846.3010 - val_loss: 224410678.5978\n",
      "Epoch 468/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 571428371.5568 - val_loss: 219268562.1845\n",
      "Epoch 469/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 470275874.9437 - val_loss: 248848842.3764\n",
      "Epoch 470/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 435035240.2364 - val_loss: 253988109.0627\n",
      "Epoch 471/500\n",
      "4332/4332 [==============================] - 0s 62us/step - loss: 548261547.6122 - val_loss: 293762504.1771\n",
      "Epoch 472/500\n",
      "4332/4332 [==============================] - 0s 75us/step - loss: 515409533.8006 - val_loss: 226788585.9926\n",
      "Epoch 473/500\n",
      "4332/4332 [==============================] - 0s 66us/step - loss: 587384567.0249 - val_loss: 207687273.0923\n",
      "Epoch 474/500\n",
      "4332/4332 [==============================] - 0s 83us/step - loss: 585964931.5439 - val_loss: 236276918.2878\n",
      "Epoch 475/500\n",
      "4332/4332 [==============================] - 0s 73us/step - loss: 553269805.0194 - val_loss: 684782767.0554\n",
      "Epoch 476/500\n",
      "4332/4332 [==============================] - 0s 74us/step - loss: 505544866.8809 - val_loss: 217462876.3395\n",
      "Epoch 477/500\n",
      "4332/4332 [==============================] - 0s 76us/step - loss: 404589569.0268 - val_loss: 225862804.5314\n",
      "Epoch 478/500\n",
      "4332/4332 [==============================] - 0s 72us/step - loss: 520404646.7959 - val_loss: 220166118.5387\n",
      "Epoch 479/500\n",
      "4332/4332 [==============================] - 0s 73us/step - loss: 450427693.7359 - val_loss: 384191694.3173\n",
      "Epoch 480/500\n",
      "4332/4332 [==============================] - 0s 72us/step - loss: 503836838.2678 - val_loss: 223627119.1587\n",
      "Epoch 481/500\n",
      "4332/4332 [==============================] - 0s 71us/step - loss: 436395692.8532 - val_loss: 303858238.7011\n",
      "Epoch 482/500\n",
      "4332/4332 [==============================] - 0s 67us/step - loss: 492454798.3712 - val_loss: 214439921.6531\n",
      "Epoch 483/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 511453537.2632 - val_loss: 233412506.0959\n",
      "Epoch 484/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 498148908.3712 - val_loss: 253325536.4723\n",
      "Epoch 485/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 507707362.6833 - val_loss: 308744293.0185\n",
      "Epoch 486/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 465391086.6482 - val_loss: 209850721.6531\n",
      "Epoch 487/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 520765692.0018 - val_loss: 216857218.7601\n",
      "Epoch 488/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 418299900.1256 - val_loss: 1160831264.0886\n",
      "Epoch 489/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 655434645.8283 - val_loss: 290671908.4723\n",
      "Epoch 490/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 448843664.6131 - val_loss: 206341163.7343\n",
      "Epoch 491/500\n",
      "4332/4332 [==============================] - 0s 63us/step - loss: 467006226.4155 - val_loss: 231064498.9373\n",
      "Epoch 492/500\n",
      "4332/4332 [==============================] - 0s 58us/step - loss: 503760620.0406 - val_loss: 237975246.5092\n",
      "Epoch 493/500\n",
      "4332/4332 [==============================] - 0s 61us/step - loss: 467145233.2428 - val_loss: 302484735.4391\n",
      "Epoch 494/500\n",
      "4332/4332 [==============================] - 0s 57us/step - loss: 459674866.5559 - val_loss: 230102905.5498\n",
      "Epoch 495/500\n",
      "4332/4332 [==============================] - 0s 59us/step - loss: 548114734.6039 - val_loss: 212673679.6310\n",
      "Epoch 496/500\n",
      "4332/4332 [==============================] - 0s 60us/step - loss: 462924953.1487 - val_loss: 215648732.7675\n",
      "Epoch 497/500\n",
      "4332/4332 [==============================] - 0s 76us/step - loss: 470353925.9649 - val_loss: 224732825.3137\n",
      "Epoch 498/500\n",
      "4332/4332 [==============================] - 0s 74us/step - loss: 449686990.8772 - val_loss: 229597757.8155\n",
      "Epoch 499/500\n",
      "4332/4332 [==============================] - 0s 75us/step - loss: 472808165.2927 - val_loss: 224272105.4317\n",
      "Epoch 500/500\n",
      "4332/4332 [==============================] - 0s 71us/step - loss: 453985216.6168 - val_loss: 279923982.0812\n"
     ]
    }
   ],
   "source": [
    "#We then finally fit the model with the training dataset, with validation spilt being kept at 20% of the training dataset.\n",
    "H1 = model.fit(X_train, y_train, verbose=1, epochs=500, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We plot the loss function (MSE) for both the actual training model and the validation dataset\n",
    "plt.plot(H1.history[\"loss\"])\n",
    "plt.plot(H1.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2668/2668 [==============================] - 0s 48us/step\n",
      "268870483.04347825\n",
      "The accuracy of the model: 0.5203332989613565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16397.270555592895"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The final evaluation score for the predicted ANN is around 52.03% with the above model\n",
    "print(\"The MSE value for the model:\", model.evaluate(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"The accuracy of the model:\", np.mean(np.abs(1-y_pred/y_test)))\n",
    "\n",
    "#We then calculate using the formula to calcualte the RMSE value\n",
    "rms1 = sqrt(mean_squared_error(y_test, y_pred))\n",
    "rms1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above model is not so accurate, hence we increase the intital door hidden layer node to be more than the input dimension\n",
    "# of 865, then we run the same process as above for the much DNN.\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(1024, input_dim=865, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(16, activation='relu'))\n",
    "model1.add(Dense(8, activation='relu'))\n",
    "model1.add(Dense(4, activation='relu'))\n",
    "model1.add(Dense(2, activation='relu'))\n",
    "model1.add(Dense(1, activation='linear'))\n",
    "\n",
    "model1.compile(loss='mean_squared_error', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4332 samples, validate on 1084 samples\n",
      "Epoch 1/500\n",
      "4332/4332 [==============================] - 4s 884us/step - loss: 8567774020.7867 - val_loss: 5751432738.0074\n",
      "Epoch 2/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8567226822.5596 - val_loss: 5751428445.5203\n",
      "Epoch 3/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8567222143.8818 - val_loss: 5751424503.4982\n",
      "Epoch 4/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8567218392.9972 - val_loss: 5751419392.0000\n",
      "Epoch 5/500\n",
      "4332/4332 [==============================] - 3s 582us/step - loss: 8567213454.0646 - val_loss: 5751415280.8856\n",
      "Epoch 6/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8567209276.2770 - val_loss: 5751410795.6900\n",
      "Epoch 7/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8567204968.7165 - val_loss: 5751406759.2030\n",
      "Epoch 8/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8567200098.9252 - val_loss: 5751402635.8081\n",
      "Epoch 9/500\n",
      "4332/4332 [==============================] - 3s 590us/step - loss: 8567196234.9326 - val_loss: 5751397563.9852\n",
      "Epoch 10/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8567191888.8421 - val_loss: 5751393810.8930\n",
      "Epoch 11/500\n",
      "4332/4332 [==============================] - 3s 588us/step - loss: 8567186970.7110 - val_loss: 5751389042.3026\n",
      "Epoch 12/500\n",
      "4332/4332 [==============================] - 3s 593us/step - loss: 8567183104.4728 - val_loss: 5751384911.3506\n",
      "Epoch 13/500\n",
      "4332/4332 [==============================] - 3s 686us/step - loss: 8567178529.0933 - val_loss: 5751380666.0959\n",
      "Epoch 14/500\n",
      "4332/4332 [==============================] - 3s 579us/step - loss: 8567173929.3666 - val_loss: 5751376346.2140\n",
      "Epoch 15/500\n",
      "4332/4332 [==============================] - 2s 576us/step - loss: 8567169931.4644 - val_loss: 5751372124.5756\n",
      "Epoch 16/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8567165158.9437 - val_loss: 5751367521.2989\n",
      "Epoch 17/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8567160736.2659 - val_loss: 5751363185.3579\n",
      "Epoch 18/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8567156629.6288 - val_loss: 5751358976.0000\n",
      "Epoch 19/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8567152133.5549 - val_loss: 5751354380.2804\n",
      "Epoch 20/500\n",
      "4332/4332 [==============================] - 3s 651us/step - loss: 8567147774.5817 - val_loss: 5751350107.6310\n",
      "Epoch 21/500\n",
      "4332/4332 [==============================] - 3s 668us/step - loss: 8567143171.0729 - val_loss: 5751346027.6900\n",
      "Epoch 22/500\n",
      "4332/4332 [==============================] - 3s 636us/step - loss: 8567139023.0693 - val_loss: 5751341448.0295\n",
      "Epoch 23/500\n",
      "4332/4332 [==============================] - 3s 630us/step - loss: 8567134647.4312 - val_loss: 5751337179.1587\n",
      "Epoch 24/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8567130231.8449 - val_loss: 5751332835.6605\n",
      "Epoch 25/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8567126010.5633 - val_loss: 5751328495.9410\n",
      "Epoch 26/500\n",
      "4332/4332 [==============================] - 3s 653us/step - loss: 8567121426.0831 - val_loss: 5751324270.5240\n",
      "Epoch 27/500\n",
      "4332/4332 [==============================] - 3s 648us/step - loss: 8567117129.5143 - val_loss: 5751319521.7712\n",
      "Epoch 28/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8567112647.9778 - val_loss: 5751315284.0738\n",
      "Epoch 29/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8567108175.4238 - val_loss: 5751310869.7269\n",
      "Epoch 30/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8567104180.5946 - val_loss: 5751306648.0886\n",
      "Epoch 31/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8567099204.1958 - val_loss: 5751302442.5092\n",
      "Epoch 32/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8567095015.6528 - val_loss: 5751297728.7085\n",
      "Epoch 33/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8567091012.7867 - val_loss: 5751293735.6753\n",
      "Epoch 34/500\n",
      "4332/4332 [==============================] - 3s 641us/step - loss: 8567086185.7802 - val_loss: 5751289277.8745\n",
      "Epoch 35/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8567082075.2428 - val_loss: 5751284938.1550: 0s - loss: 9\n",
      "Epoch 36/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8567077468.6611 - val_loss: 5751280669.2841\n",
      "Epoch 37/500\n",
      "4332/4332 [==============================] - 3s 650us/step - loss: 8567072788.3287 - val_loss: 5751275857.2399\n",
      "Epoch 38/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8567068652.6168 - val_loss: 5751272076.7528\n",
      "Epoch 39/500\n",
      "4332/4332 [==============================] - 3s 647us/step - loss: 8567064372.7128 - val_loss: 5751267733.2546\n",
      "Epoch 40/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8567059720.1551 - val_loss: 5751263015.6753\n",
      "Epoch 41/500\n",
      "4332/4332 [==============================] - 3s 648us/step - loss: 8567055414.3675 - val_loss: 5751258675.9557\n",
      "Epoch 42/500\n",
      "4332/4332 [==============================] - 3s 637us/step - loss: 8567051124.5356 - val_loss: 5751254545.0037\n",
      "Epoch 43/500\n",
      "4332/4332 [==============================] - 3s 650us/step - loss: 8567046505.1893 - val_loss: 5751250465.0627\n",
      "Epoch 44/500\n",
      "4332/4332 [==============================] - 3s 662us/step - loss: 8567042191.9557 - val_loss: 5751245814.5535\n",
      "Epoch 45/500\n",
      "4332/4332 [==============================] - 3s 660us/step - loss: 8567037576.1551 - val_loss: 5751241289.6827\n",
      "Epoch 46/500\n",
      "4332/4332 [==============================] - 3s 641us/step - loss: 8567033515.8486 - val_loss: 5751237391.1144\n",
      "Epoch 47/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8567029432.3767 - val_loss: 5751232531.8376\n",
      "Epoch 48/500\n",
      "4332/4332 [==============================] - 3s 666us/step - loss: 8567024779.7008 - val_loss: 5751228518.9668\n",
      "Epoch 49/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8567020306.5559 - val_loss: 5751224108.3985\n",
      "Epoch 50/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8567016521.5143 - val_loss: 5751219508.9004\n",
      "Epoch 51/500\n",
      "4332/4332 [==============================] - 3s 636us/step - loss: 8567011849.4552 - val_loss: 5751215614.1107\n",
      "Epoch 52/500\n",
      "4332/4332 [==============================] - 3s 641us/step - loss: 8567007187.7969 - val_loss: 5751210987.2177\n",
      "Epoch 53/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8567002940.7498 - val_loss: 5751206553.0332\n",
      "Epoch 54/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566998287.1283 - val_loss: 5751202213.3137\n",
      "Epoch 55/500\n",
      "4332/4332 [==============================] - 3s 650us/step - loss: 8566993964.9123 - val_loss: 5751197846.1993\n",
      "Epoch 56/500\n",
      "4332/4332 [==============================] - 3s 662us/step - loss: 8566989404.6611 - val_loss: 5751193837.1070\n",
      "Epoch 57/500\n",
      "4332/4332 [==============================] - 3s 658us/step - loss: 8566985074.6445 - val_loss: 5751189115.7491\n",
      "Epoch 58/500\n",
      "4332/4332 [==============================] - 3s 661us/step - loss: 8566980800.1773 - val_loss: 5751184681.5646\n",
      "Epoch 59/500\n",
      "4332/4332 [==============================] - 3s 647us/step - loss: 8566976310.1311 - val_loss: 5751180645.0775\n",
      "Epoch 60/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8566971816.0665 - val_loss: 5751176021.9631\n",
      "Epoch 61/500\n",
      "4332/4332 [==============================] - 2s 552us/step - loss: 8566967537.3444 - val_loss: 5751171843.7786\n",
      "Epoch 62/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8566963093.7470 - val_loss: 5751167692.9889\n",
      "Epoch 63/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8566959091.1173 - val_loss: 5751162735.4686\n",
      "Epoch 64/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566954585.9428 - val_loss: 5751158773.6089\n",
      "Epoch 65/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8566949866.0166 - val_loss: 5751154312.0295\n",
      "Epoch 66/500\n",
      "4332/4332 [==============================] - 3s 583us/step - loss: 8566945328.2216 - val_loss: 5751150157.4613\n",
      "Epoch 67/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566941060.3730 - val_loss: 5751145723.2768\n",
      "Epoch 68/500\n",
      "4332/4332 [==============================] - 3s 607us/step - loss: 8566936868.7572 - val_loss: 5751141359.9410\n",
      "Epoch 69/500\n",
      "4332/4332 [==============================] - 3s 581us/step - loss: 8566932455.1801 - val_loss: 5751137205.3727\n",
      "Epoch 70/500\n",
      "4332/4332 [==============================] - 3s 584us/step - loss: 8566927906.7479 - val_loss: 5751132484.0148\n",
      "Epoch 71/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8566923535.3647 - val_loss: 5751128289.7712\n",
      "Epoch 72/500\n",
      "4332/4332 [==============================] - 3s 593us/step - loss: 8566919201.5660 - val_loss: 5751123969.8893\n",
      "Epoch 73/500\n",
      "4332/4332 [==============================] - 3s 599us/step - loss: 8566914918.8255 - val_loss: 5751119417.6236\n",
      "Epoch 74/500\n",
      "4332/4332 [==============================] - 3s 578us/step - loss: 8566910280.8052 - val_loss: 5751115357.5203\n",
      "Epoch 75/500\n",
      "4332/4332 [==============================] - 2s 559us/step - loss: 8566906163.5309 - val_loss: 5751110498.2435\n",
      "Epoch 76/500\n",
      "4332/4332 [==============================] - 3s 588us/step - loss: 8566901505.1819 - val_loss: 5751106528.8266\n",
      "Epoch 77/500\n",
      "4332/4332 [==============================] - 3s 596us/step - loss: 8566897071.3943 - val_loss: 5751102240.1181\n",
      "Epoch 78/500\n",
      "4332/4332 [==============================] - 3s 589us/step - loss: 8566892636.0702 - val_loss: 5751097636.8413\n",
      "Epoch 79/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8566888229.3481 - val_loss: 5751093576.7380\n",
      "Epoch 80/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8566883856.3102 - val_loss: 5751088811.9262\n",
      "Epoch 81/500\n",
      "4332/4332 [==============================] - 3s 632us/step - loss: 8566879377.4922 - val_loss: 5751084539.2768\n",
      "Epoch 82/500\n",
      "4332/4332 [==============================] - 3s 632us/step - loss: 8566875229.0157 - val_loss: 5751080270.4059\n",
      "Epoch 83/500\n",
      "4332/4332 [==============================] - 3s 629us/step - loss: 8566870694.4118 - val_loss: 5751075836.2214\n",
      "Epoch 84/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566866501.0231 - val_loss: 5751071799.7343\n",
      "Epoch 85/500\n",
      "4332/4332 [==============================] - 3s 629us/step - loss: 8566861939.7082 - val_loss: 5751067318.3173\n",
      "Epoch 86/500\n",
      "4332/4332 [==============================] - 3s 659us/step - loss: 8566857436.3066 - val_loss: 5751062833.1218\n",
      "Epoch 87/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8566853090.2161 - val_loss: 5751058517.0184\n",
      "Epoch 88/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8566848756.2992 - val_loss: 5751054244.3690\n",
      "Epoch 89/500\n",
      "4332/4332 [==============================] - 3s 637us/step - loss: 8566844053.8652 - val_loss: 5751050184.2657\n",
      "Epoch 90/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8566840137.7507 - val_loss: 5751045726.4649\n",
      "Epoch 91/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566835528.3324 - val_loss: 5751041056.1181\n",
      "Epoch 92/500\n",
      "4332/4332 [==============================] - 3s 652us/step - loss: 8566831167.3500 - val_loss: 5751037397.4908\n",
      "Epoch 93/500\n",
      "4332/4332 [==============================] - 3s 663us/step - loss: 8566827107.0434 - val_loss: 5751032345.5055\n",
      "Epoch 94/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566822445.8578 - val_loss: 5751028123.8672\n",
      "Epoch 95/500\n",
      "4332/4332 [==============================] - 3s 663us/step - loss: 8566817923.9003 - val_loss: 5751023729.3579\n",
      "Epoch 96/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566814068.7719 - val_loss: 5751019346.1845\n",
      "Epoch 97/500\n",
      "4332/4332 [==============================] - 3s 656us/step - loss: 8566809249.9206 - val_loss: 5751015262.4649\n",
      "Epoch 98/500\n",
      "4332/4332 [==============================] - 3s 640us/step - loss: 8566804951.8153 - val_loss: 5751011088.0590\n",
      "Epoch 99/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566800627.7082 - val_loss: 5751006370.4797\n",
      "Epoch 100/500\n",
      "4332/4332 [==============================] - 3s 640us/step - loss: 8566796250.4155 - val_loss: 5751001956.1328\n",
      "Epoch 101/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566791986.5854 - val_loss: 5750997750.5535\n",
      "Epoch 102/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566787293.9612 - val_loss: 5750993390.9963\n",
      "Epoch 103/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8566782876.2475 - val_loss: 5750989291.2177\n",
      "Epoch 104/500\n",
      "4332/4332 [==============================] - 3s 657us/step - loss: 8566778426.8587 - val_loss: 5750984684.1624\n",
      "Epoch 105/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8566774388.0628 - val_loss: 5750980580.6052\n",
      "Epoch 106/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566769608.2142 - val_loss: 5750975977.3284\n",
      "Epoch 107/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8566765373.9317 - val_loss: 5750971799.1439\n",
      "Epoch 108/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8566761200.6353 - val_loss: 5750967412.1919\n",
      "Epoch 109/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8566756556.4691 - val_loss: 5750962623.7638\n",
      "Epoch 110/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566752261.2004 - val_loss: 5750958705.3579\n",
      "Epoch 111/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566747817.2484 - val_loss: 5750954031.2325\n",
      "Epoch 112/500\n",
      "4332/4332 [==============================] - 3s 654us/step - loss: 8566743189.3924 - val_loss: 5750949880.4428\n",
      "Epoch 113/500\n",
      "4332/4332 [==============================] - 3s 624us/step - loss: 8566738834.5559 - val_loss: 5750945638.9668\n",
      "Epoch 114/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566734826.9621 - val_loss: 5750940937.4465\n",
      "Epoch 115/500\n",
      "4332/4332 [==============================] - 3s 645us/step - loss: 8566730141.1930 - val_loss: 5750936995.4244\n",
      "Epoch 116/500\n",
      "4332/4332 [==============================] - 3s 655us/step - loss: 8566726149.2004 - val_loss: 5750932604.6937\n",
      "Epoch 117/500\n",
      "4332/4332 [==============================] - 3s 619us/step - loss: 8566721417.5734 - val_loss: 5750927985.3579\n",
      "Epoch 118/500\n",
      "4332/4332 [==============================] - 3s 627us/step - loss: 8566717186.6002 - val_loss: 5750923787.3358\n",
      "Epoch 119/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566712690.6445 - val_loss: 5750919420.2214\n",
      "Epoch 120/500\n",
      "4332/4332 [==============================] - 3s 597us/step - loss: 8566708139.3758 - val_loss: 5750915174.9668\n",
      "Epoch 121/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8566703590.3527 - val_loss: 5750910449.8303\n",
      "Epoch 122/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8566699546.0018 - val_loss: 5750906255.5867\n",
      "Epoch 123/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8566694817.4478 - val_loss: 5750901888.4723\n",
      "Epoch 124/500\n",
      "4332/4332 [==============================] - 3s 586us/step - loss: 8566690246.0868 - val_loss: 5750897426.8930\n",
      "Epoch 125/500\n",
      "4332/4332 [==============================] - 2s 554us/step - loss: 8566686358.4561 - val_loss: 5750893441.4170\n",
      "Epoch 126/500\n",
      "4332/4332 [==============================] - 3s 577us/step - loss: 8566681541.3777 - val_loss: 5750889054.4649\n",
      "Epoch 127/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8566677396.9197 - val_loss: 5750884522.0369\n",
      "Epoch 128/500\n",
      "4332/4332 [==============================] - 3s 621us/step - loss: 8566673288.3915 - val_loss: 5750880158.7011\n",
      "Epoch 129/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566668129.1524 - val_loss: 5750875622.4945\n",
      "Epoch 130/500\n",
      "4332/4332 [==============================] - 3s 650us/step - loss: 8566664270.0055 - val_loss: 5750871711.6458\n",
      "Epoch 131/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566660046.1237 - val_loss: 5750867411.6015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "4332/4332 [==============================] - 3s 600us/step - loss: 8566655343.0988 - val_loss: 5750862508.8708\n",
      "Epoch 133/500\n",
      "4332/4332 [==============================] - 3s 603us/step - loss: 8566650938.1496 - val_loss: 5750858614.0812\n",
      "Epoch 134/500\n",
      "4332/4332 [==============================] - 3s 614us/step - loss: 8566646742.3970 - val_loss: 5750854058.0369\n",
      "Epoch 135/500\n",
      "4332/4332 [==============================] - 3s 608us/step - loss: 8566642151.1801 - val_loss: 5750849722.0959\n",
      "Epoch 136/500\n",
      "4332/4332 [==============================] - 3s 610us/step - loss: 8566637932.3804 - val_loss: 5750845567.5277\n",
      "Epoch 137/500\n",
      "4332/4332 [==============================] - 3s 612us/step - loss: 8566633407.3500 - val_loss: 5750840893.4022\n",
      "Epoch 138/500\n",
      "4332/4332 [==============================] - 3s 623us/step - loss: 8566628907.6122 - val_loss: 5750836738.8339\n",
      "Epoch 139/500\n",
      "4332/4332 [==============================] - 3s 627us/step - loss: 8566624639.6454 - val_loss: 5750832517.1956\n",
      "Epoch 140/500\n",
      "4332/4332 [==============================] - 3s 624us/step - loss: 8566620239.7784 - val_loss: 5750828126.4649\n",
      "Epoch 141/500\n",
      "4332/4332 [==============================] - 3s 607us/step - loss: 8566615799.2539 - val_loss: 5750823668.6642\n",
      "Epoch 142/500\n",
      "4332/4332 [==============================] - 3s 620us/step - loss: 8566611751.7119 - val_loss: 5750819183.4686\n",
      "Epoch 143/500\n",
      "4332/4332 [==============================] - 3s 623us/step - loss: 8566606765.6214 - val_loss: 5750815150.7601\n",
      "Epoch 144/500\n",
      "4332/4332 [==============================] - 3s 615us/step - loss: 8566602465.0342 - val_loss: 5750810807.2620\n",
      "Epoch 145/500\n",
      "4332/4332 [==============================] - 3s 632us/step - loss: 8566598499.6343 - val_loss: 5750806117.0775\n",
      "Epoch 146/500\n",
      "4332/4332 [==============================] - 3s 616us/step - loss: 8566593901.3259 - val_loss: 5750801935.1144\n",
      "Epoch 147/500\n",
      "4332/4332 [==============================] - 3s 628us/step - loss: 8566589326.0646 - val_loss: 5750797619.0111\n",
      "Epoch 148/500\n",
      "4332/4332 [==============================] - 3s 620us/step - loss: 8566584932.8163 - val_loss: 5750793491.8376\n",
      "Epoch 149/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8566580470.3084 - val_loss: 5750788939.5720\n",
      "Epoch 150/500\n",
      "4332/4332 [==============================] - 3s 619us/step - loss: 8566576310.7221 - val_loss: 5750784147.3653\n",
      "Epoch 151/500\n",
      "4332/4332 [==============================] - 3s 629us/step - loss: 8566571923.0286 - val_loss: 5750780205.3432\n",
      "Epoch 152/500\n",
      "4332/4332 [==============================] - 3s 591us/step - loss: 8566567464.4211 - val_loss: 5750775723.9262\n",
      "Epoch 153/500\n",
      "4332/4332 [==============================] - 3s 582us/step - loss: 8566563124.0037 - val_loss: 5750771376.6494\n",
      "Epoch 154/500\n",
      "4332/4332 [==============================] - 2s 576us/step - loss: 8566558855.2096 - val_loss: 5750767111.5572\n",
      "Epoch 155/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566554077.2521 - val_loss: 5750762697.2103\n",
      "Epoch 156/500\n",
      "4332/4332 [==============================] - 3s 584us/step - loss: 8566549873.6990 - val_loss: 5750758755.1882\n",
      "Epoch 157/500\n",
      "4332/4332 [==============================] - 3s 589us/step - loss: 8566545622.6334 - val_loss: 5750753892.1328\n",
      "Epoch 158/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8566541034.0166 - val_loss: 5750749583.5867\n",
      "Epoch 159/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566536751.2761 - val_loss: 5750745425.2399\n",
      "Epoch 160/500\n",
      "4332/4332 [==============================] - 3s 612us/step - loss: 8566532313.4700 - val_loss: 5750741061.9041\n",
      "Epoch 161/500\n",
      "4332/4332 [==============================] - 3s 653us/step - loss: 8566527955.0877 - val_loss: 5750736856.3247\n",
      "Epoch 162/500\n",
      "4332/4332 [==============================] - 3s 639us/step - loss: 8566523868.3066 - val_loss: 5750732118.9077\n",
      "Epoch 163/500\n",
      "4332/4332 [==============================] - 3s 667us/step - loss: 8566518999.2244 - val_loss: 5750727802.8044\n",
      "Epoch 164/500\n",
      "4332/4332 [==============================] - 3s 666us/step - loss: 8566514705.9649 - val_loss: 5750723695.4686\n",
      "Epoch 165/500\n",
      "4332/4332 [==============================] - 3s 645us/step - loss: 8566510572.8532 - val_loss: 5750719375.5867\n",
      "Epoch 166/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8566505990.3823 - val_loss: 5750715059.4834\n",
      "Epoch 167/500\n",
      "4332/4332 [==============================] - 3s 640us/step - loss: 8566501687.0766 - val_loss: 5750710743.3801\n",
      "Epoch 168/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566497311.6750 - val_loss: 5750706258.1845\n",
      "Epoch 169/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566492474.6223 - val_loss: 5750702080.0000\n",
      "Epoch 170/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8566488423.0619 - val_loss: 5750697240.5609\n",
      "Epoch 171/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566484210.0536 - val_loss: 5750693255.0849\n",
      "Epoch 172/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566479222.4266 - val_loss: 5750688895.5277\n",
      "Epoch 173/500\n",
      "4332/4332 [==============================] - 3s 645us/step - loss: 8566475128.0813 - val_loss: 5750684292.2509\n",
      "Epoch 174/500\n",
      "4332/4332 [==============================] - 3s 667us/step - loss: 8566470749.1339 - val_loss: 5750680464.5314\n",
      "Epoch 175/500\n",
      "4332/4332 [==============================] - 3s 627us/step - loss: 8566466122.8144 - val_loss: 5750675719.5572\n",
      "Epoch 176/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8566461821.1634 - val_loss: 5750671360.0000\n",
      "Epoch 177/500\n",
      "4332/4332 [==============================] - 3s 649us/step - loss: 8566457608.9825 - val_loss: 5750667020.2804\n",
      "Epoch 178/500\n",
      "4332/4332 [==============================] - 3s 640us/step - loss: 8566453335.2244 - val_loss: 5750662535.0849\n",
      "Epoch 179/500\n",
      "4332/4332 [==============================] - 3s 649us/step - loss: 8566448922.9474 - val_loss: 5750658474.9816\n",
      "Epoch 180/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566444552.9825 - val_loss: 5750653926.4945\n",
      "Epoch 181/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8566439641.2336 - val_loss: 5750649464.9151\n",
      "Epoch 182/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8566435671.4608 - val_loss: 5750645357.5793\n",
      "Epoch 183/500\n",
      "4332/4332 [==============================] - 3s 636us/step - loss: 8566430981.6731 - val_loss: 5750640896.0000\n",
      "Epoch 184/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8566426712.1699 - val_loss: 5750636816.0590\n",
      "Epoch 185/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566422163.2650 - val_loss: 5750632074.8635\n",
      "Epoch 186/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566418126.3601 - val_loss: 5750627707.7491\n",
      "Epoch 187/500\n",
      "4332/4332 [==============================] - 3s 657us/step - loss: 8566413479.5937 - val_loss: 5750623482.3321\n",
      "Epoch 188/500\n",
      "4332/4332 [==============================] - 3s 666us/step - loss: 8566409359.9557 - val_loss: 5750619280.5314\n",
      "Epoch 189/500\n",
      "4332/4332 [==============================] - 3s 669us/step - loss: 8566404737.5365 - val_loss: 5750614799.1144\n",
      "Epoch 190/500\n",
      "4332/4332 [==============================] - 3s 663us/step - loss: 8566400182.9584 - val_loss: 5750610486.7897\n",
      "Epoch 191/500\n",
      "4332/4332 [==============================] - 3s 663us/step - loss: 8566396218.6223 - val_loss: 5750605974.1993\n",
      "Epoch 192/500\n",
      "4332/4332 [==============================] - 3s 668us/step - loss: 8566391526.4709 - val_loss: 5750602059.5720\n",
      "Epoch 193/500\n",
      "4332/4332 [==============================] - 3s 647us/step - loss: 8566387516.0406 - val_loss: 5750597432.6790\n",
      "Epoch 194/500\n",
      "4332/4332 [==============================] - 3s 648us/step - loss: 8566382688.2068 - val_loss: 5750593092.9594\n",
      "Epoch 195/500\n",
      "4332/4332 [==============================] - 3s 628us/step - loss: 8566378508.6464 - val_loss: 5750588611.5424\n",
      "Epoch 196/500\n",
      "4332/4332 [==============================] - 3s 640us/step - loss: 8566374016.3546 - val_loss: 5750584130.1255\n",
      "Epoch 197/500\n",
      "4332/4332 [==============================] - 3s 608us/step - loss: 8566369887.9705 - val_loss: 5750580046.4059\n",
      "Epoch 198/500\n",
      "4332/4332 [==============================] - 3s 600us/step - loss: 8566365060.8458 - val_loss: 5750575722.7454\n",
      "Epoch 199/500\n",
      "4332/4332 [==============================] - 3s 596us/step - loss: 8566360803.3980 - val_loss: 5750571154.4207\n",
      "Epoch 200/500\n",
      "4332/4332 [==============================] - 3s 607us/step - loss: 8566356588.2622 - val_loss: 5750567043.3063\n",
      "Epoch 201/500\n",
      "4332/4332 [==============================] - 3s 605us/step - loss: 8566352278.3379 - val_loss: 5750562231.2620\n",
      "Epoch 202/500\n",
      "4332/4332 [==============================] - 3s 618us/step - loss: 8566347878.1163 - val_loss: 5750558430.9373\n",
      "Epoch 203/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8566343473.4035 - val_loss: 5750553709.5793\n",
      "Epoch 204/500\n",
      "4332/4332 [==============================] - 3s 582us/step - loss: 8566338690.7184 - val_loss: 5750549676.8708\n",
      "Epoch 205/500\n",
      "4332/4332 [==============================] - 3s 579us/step - loss: 8566334589.7544 - val_loss: 5750545171.8376\n",
      "Epoch 206/500\n",
      "4332/4332 [==============================] - 3s 587us/step - loss: 8566329893.2299 - val_loss: 5750541013.4908\n",
      "Epoch 207/500\n",
      "4332/4332 [==============================] - 3s 592us/step - loss: 8566325472.5614 - val_loss: 5750536441.3875\n",
      "Epoch 208/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8566321453.8578 - val_loss: 5750531747.4244\n",
      "Epoch 209/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8566316676.3730 - val_loss: 5750527710.9373\n",
      "Epoch 210/500\n",
      "4332/4332 [==============================] - 3s 583us/step - loss: 8566312612.1662 - val_loss: 5750523556.3690\n",
      "Epoch 211/500\n",
      "4332/4332 [==============================] - 3s 602us/step - loss: 8566307931.9520 - val_loss: 5750518953.0923\n",
      "Epoch 212/500\n",
      "4332/4332 [==============================] - 3s 645us/step - loss: 8566303530.9030 - val_loss: 5750514825.9188\n",
      "Epoch 213/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566299354.5337 - val_loss: 5750510557.0480\n",
      "Epoch 214/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8566294878.5522 - val_loss: 5750505863.0849\n",
      "Epoch 215/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566290476.9123 - val_loss: 5750501637.6679\n",
      "Epoch 216/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566286100.3287 - val_loss: 5750497176.0886\n",
      "Epoch 217/500\n",
      "4332/4332 [==============================] - 3s 626us/step - loss: 8566281952.3250 - val_loss: 5750493163.2177\n",
      "Epoch 218/500\n",
      "4332/4332 [==============================] - 3s 630us/step - loss: 8566277173.8947 - val_loss: 5750488685.5793\n",
      "Epoch 219/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8566272802.0388 - val_loss: 5750484267.4539\n",
      "Epoch 220/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8566268553.8098 - val_loss: 5750479955.1292\n",
      "Epoch 221/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8566263778.6888 - val_loss: 5750475540.7823\n",
      "Epoch 222/500\n",
      "4332/4332 [==============================] - 3s 627us/step - loss: 8566259787.5235 - val_loss: 5750471201.0627\n",
      "Epoch 223/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566255375.8375 - val_loss: 5750466952.0295\n",
      "Epoch 224/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566250936.6131 - val_loss: 5750462230.6716\n",
      "Epoch 225/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566246474.4598 - val_loss: 5750457871.1144\n",
      "Epoch 226/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566242066.9104 - val_loss: 5750453429.3727\n",
      "Epoch 227/500\n",
      "4332/4332 [==============================] - 3s 645us/step - loss: 8566237790.7886 - val_loss: 5750449136.8856\n",
      "Epoch 228/500\n",
      "4332/4332 [==============================] - 3s 647us/step - loss: 8566233404.8680 - val_loss: 5750445009.7122\n",
      "Epoch 229/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8566228975.4534 - val_loss: 5750440807.9114\n",
      "Epoch 230/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8566224235.0803 - val_loss: 5750436515.4244\n",
      "Epoch 231/500\n",
      "4332/4332 [==============================] - 3s 649us/step - loss: 8566220139.6713 - val_loss: 5750431770.4502\n",
      "Epoch 232/500\n",
      "4332/4332 [==============================] - 3s 636us/step - loss: 8566215615.2318 - val_loss: 5750427450.5683\n",
      "Epoch 233/500\n",
      "4332/4332 [==============================] - 3s 654us/step - loss: 8566211421.1339 - val_loss: 5750423417.8598\n",
      "Epoch 234/500\n",
      "4332/4332 [==============================] - 3s 677us/step - loss: 8566207017.6030 - val_loss: 5750418861.8155\n",
      "Epoch 235/500\n",
      "4332/4332 [==============================] - 3s 651us/step - loss: 8566202617.7359 - val_loss: 5750414502.2583\n",
      "Epoch 236/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8566198085.2595 - val_loss: 5750410347.6900\n",
      "Epoch 237/500\n",
      "4332/4332 [==============================] - 3s 649us/step - loss: 8566193861.9686 - val_loss: 5750405649.9483\n",
      "Epoch 238/500\n",
      "4332/4332 [==============================] - 3s 644us/step - loss: 8566189348.4026 - val_loss: 5750401735.3210\n",
      "Epoch 239/500\n",
      "4332/4332 [==============================] - 3s 645us/step - loss: 8566184919.5789 - val_loss: 5750397037.5793\n",
      "Epoch 240/500\n",
      "4332/4332 [==============================] - 3s 648us/step - loss: 8566180695.4608 - val_loss: 5750392934.0221\n",
      "Epoch 241/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8566176115.5900 - val_loss: 5750388358.1402\n",
      "Epoch 242/500\n",
      "4332/4332 [==============================] - 3s 654us/step - loss: 8566171702.1311 - val_loss: 5750383920.1771\n",
      "Epoch 243/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566167701.3924 - val_loss: 5750379812.8413\n",
      "Epoch 244/500\n",
      "4332/4332 [==============================] - 3s 625us/step - loss: 8566162976.7387 - val_loss: 5750375169.8893\n",
      "Epoch 245/500\n",
      "4332/4332 [==============================] - 3s 649us/step - loss: 8566158605.7101 - val_loss: 5750371011.5424\n",
      "Epoch 246/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8566154397.9021 - val_loss: 5750366907.9852\n",
      "Epoch 247/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8566149783.6380 - val_loss: 5750362304.7085\n",
      "Epoch 248/500\n",
      "4332/4332 [==============================] - 3s 643us/step - loss: 8566145743.7784 - val_loss: 5750358366.4649\n",
      "Epoch 249/500\n",
      "4332/4332 [==============================] - 3s 615us/step - loss: 8566140956.8384 - val_loss: 5750353314.4797\n",
      "Epoch 250/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8566136416.4432 - val_loss: 5750349140.0738\n",
      "Epoch 251/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8566132443.5974 - val_loss: 5750345150.8192\n",
      "Epoch 252/500\n",
      "4332/4332 [==============================] - 3s 587us/step - loss: 8566127815.5051 - val_loss: 5750340338.7749\n",
      "Epoch 253/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8566123523.5457 - val_loss: 5750336487.4391\n",
      "Epoch 254/500\n",
      "4332/4332 [==============================] - 2s 545us/step - loss: 8566119105.4774 - val_loss: 5750331750.0221\n",
      "Epoch 255/500\n",
      "4332/4332 [==============================] - 2s 551us/step - loss: 8566114446.5374 - val_loss: 5750327410.3026\n",
      "Epoch 256/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8566109903.8966 - val_loss: 5750323283.1292\n",
      "Epoch 257/500\n",
      "4332/4332 [==============================] - 3s 584us/step - loss: 8566106027.3758 - val_loss: 5750318959.4686\n",
      "Epoch 258/500\n",
      "4332/4332 [==============================] - 3s 577us/step - loss: 8566101252.4912 - val_loss: 5750314454.4354\n",
      "Epoch 259/500\n",
      "4332/4332 [==============================] - 2s 576us/step - loss: 8566097002.3712 - val_loss: 5750310229.0184\n",
      "Epoch 260/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8566092651.0803 - val_loss: 5750305582.2878\n",
      "Epoch 261/500\n",
      "4332/4332 [==============================] - 3s 580us/step - loss: 8566087966.1385 - val_loss: 5750301667.6605\n",
      "Epoch 262/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4332/4332 [==============================] - 2s 562us/step - loss: 8566083901.9317 - val_loss: 5750296875.4539\n",
      "Epoch 263/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8566079661.0305 - val_loss: 5750292909.8155\n",
      "Epoch 264/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8566074716.1884 - val_loss: 5750288522.8635\n",
      "Epoch 265/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8566070608.8421 - val_loss: 5750283876.1328\n",
      "Epoch 266/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8566066084.0480 - val_loss: 5750279603.4834\n",
      "Epoch 267/500\n",
      "4332/4332 [==============================] - 2s 556us/step - loss: 8566061696.8273 - val_loss: 5750275074.8339\n",
      "Epoch 268/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566057290.6962 - val_loss: 5750270943.8819\n",
      "Epoch 269/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566053023.3204 - val_loss: 5750266509.6974\n",
      "Epoch 270/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8566048707.7230 - val_loss: 5750261886.5830\n",
      "Epoch 271/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8566044085.0674 - val_loss: 5750258176.9446\n",
      "Epoch 272/500\n",
      "4332/4332 [==============================] - 3s 587us/step - loss: 8566039520.0886 - val_loss: 5750253203.3653\n",
      "Epoch 273/500\n",
      "4332/4332 [==============================] - 3s 578us/step - loss: 8566035247.7488 - val_loss: 5750248930.7159\n",
      "Epoch 274/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8566030804.2696 - val_loss: 5750244830.9373\n",
      "Epoch 275/500\n",
      "4332/4332 [==============================] - 2s 568us/step - loss: 8566026370.4820 - val_loss: 5750240507.2768\n",
      "Epoch 276/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8566022248.4801 - val_loss: 5750235978.6273\n",
      "Epoch 277/500\n",
      "4332/4332 [==============================] - 2s 559us/step - loss: 8566017973.0674 - val_loss: 5750231536.8856\n",
      "Epoch 278/500\n",
      "4332/4332 [==============================] - 2s 561us/step - loss: 8566013528.1699 - val_loss: 5750227271.7934\n",
      "Epoch 279/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8566008889.9132 - val_loss: 5750223140.8413\n",
      "Epoch 280/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8566004605.6362 - val_loss: 5750218443.0996\n",
      "Epoch 281/500\n",
      "4332/4332 [==============================] - 2s 561us/step - loss: 8565999891.3832 - val_loss: 5750214363.1587\n",
      "Epoch 282/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565995641.9723 - val_loss: 5750210070.6716\n",
      "Epoch 283/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565991633.4331 - val_loss: 5750205420.1624\n",
      "Epoch 284/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8565986977.6842 - val_loss: 5750201056.8266\n",
      "Epoch 285/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8565982565.9982 - val_loss: 5750196807.7934\n",
      "Epoch 286/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565978329.2336 - val_loss: 5750192511.5277\n",
      "Epoch 287/500\n",
      "4332/4332 [==============================] - 2s 553us/step - loss: 8565973592.5245 - val_loss: 5750188270.0517\n",
      "Epoch 288/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565969543.4460 - val_loss: 5750183619.5424\n",
      "Epoch 289/500\n",
      "4332/4332 [==============================] - 2s 561us/step - loss: 8565965041.8172 - val_loss: 5750179539.6015\n",
      "Epoch 290/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565960312.0813 - val_loss: 5750175097.8598\n",
      "Epoch 291/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8565956255.5568 - val_loss: 5750170856.3838\n",
      "Epoch 292/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565951771.6565 - val_loss: 5750166465.6531\n",
      "Epoch 293/500\n",
      "4332/4332 [==============================] - 2s 568us/step - loss: 8565947201.7138 - val_loss: 5750161842.5387\n",
      "Epoch 294/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565943266.6888 - val_loss: 5750157876.9004\n",
      "Epoch 295/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565938482.3490 - val_loss: 5750152990.2288\n",
      "Epoch 296/500\n",
      "4332/4332 [==============================] - 2s 552us/step - loss: 8565934014.7590 - val_loss: 5750148815.8229\n",
      "Epoch 297/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565930176.4137 - val_loss: 5750144735.8819\n",
      "Epoch 298/500\n",
      "4332/4332 [==============================] - 2s 555us/step - loss: 8565925169.8763 - val_loss: 5750139943.6753\n",
      "Epoch 299/500\n",
      "4332/4332 [==============================] - 2s 553us/step - loss: 8565920837.4958 - val_loss: 5750136048.8856\n",
      "Epoch 300/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565916757.5697 - val_loss: 5750131236.8413\n",
      "Epoch 301/500\n",
      "4332/4332 [==============================] - 2s 556us/step - loss: 8565912203.9372 - val_loss: 5750127227.7491\n",
      "Epoch 302/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565907745.3296 - val_loss: 5750122958.8782\n",
      "Epoch 303/500\n",
      "4332/4332 [==============================] - 2s 556us/step - loss: 8565903430.9141 - val_loss: 5750118210.1255\n",
      "Epoch 304/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565899132.0997 - val_loss: 5750114338.9520\n",
      "Epoch 305/500\n",
      "4332/4332 [==============================] - 2s 555us/step - loss: 8565894575.3943 - val_loss: 5750109999.2325\n",
      "Epoch 306/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565890112.1773 - val_loss: 5750105120.1181\n",
      "Epoch 307/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565885524.3878 - val_loss: 5750101127.0849\n",
      "Epoch 308/500\n",
      "4332/4332 [==============================] - 2s 568us/step - loss: 8565881421.0600 - val_loss: 5750096740.1328\n",
      "Epoch 309/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565877411.3389 - val_loss: 5750092463.7048\n",
      "Epoch 310/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565872438.6039 - val_loss: 5750088316.6937\n",
      "Epoch 311/500\n",
      "4332/4332 [==============================] - 3s 596us/step - loss: 8565868561.9649 - val_loss: 5750083457.4170\n",
      "Epoch 312/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565863942.0277 - val_loss: 5750079539.0111\n",
      "Epoch 313/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565859505.7581 - val_loss: 5750075053.8155\n",
      "Epoch 314/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565855391.3204 - val_loss: 5750070599.7934\n",
      "Epoch 315/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565850679.3130 - val_loss: 5750066279.9114\n",
      "Epoch 316/500\n",
      "4332/4332 [==============================] - 2s 561us/step - loss: 8565846284.0554 - val_loss: 5750061653.0184\n",
      "Epoch 317/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565841889.0342 - val_loss: 5750057758.2288\n",
      "Epoch 318/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565837750.0129 - val_loss: 5750053060.4871\n",
      "Epoch 319/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565832960.7091 - val_loss: 5750048751.9410\n",
      "Epoch 320/500\n",
      "4332/4332 [==============================] - 2s 555us/step - loss: 8565828939.8781 - val_loss: 5750044711.6753\n",
      "Epoch 321/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565824217.7064 - val_loss: 5750039852.3985\n",
      "Epoch 322/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565819908.2548 - val_loss: 5750035933.9926\n",
      "Epoch 323/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565815648.7978 - val_loss: 5750031240.0295\n",
      "Epoch 324/500\n",
      "4332/4332 [==============================] - 2s 555us/step - loss: 8565811058.1717 - val_loss: 5750026896.5314\n",
      "Epoch 325/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565806654.1681 - val_loss: 5750022745.7417\n",
      "Epoch 326/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565802635.2281 - val_loss: 5750018236.9299\n",
      "Epoch 327/500\n",
      "4332/4332 [==============================] - 3s 586us/step - loss: 8565798140.4543 - val_loss: 5750013897.2103\n",
      "Epoch 328/500\n",
      "4332/4332 [==============================] - 3s 579us/step - loss: 8565793772.6168 - val_loss: 5750009840.8856\n",
      "Epoch 329/500\n",
      "4332/4332 [==============================] - 3s 596us/step - loss: 8565789377.1228 - val_loss: 5750005119.5277\n",
      "Epoch 330/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565785137.1671 - val_loss: 5750001271.9705\n",
      "Epoch 331/500\n",
      "4332/4332 [==============================] - 3s 583us/step - loss: 8565780412.8680 - val_loss: 5749996648.8561\n",
      "Epoch 332/500\n",
      "4332/4332 [==============================] - 2s 552us/step - loss: 8565776145.6103 - val_loss: 5749992565.1365\n",
      "Epoch 333/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8565771532.2918 - val_loss: 5749988107.3358\n",
      "Epoch 334/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565767070.3749 - val_loss: 5749983436.9889\n",
      "Epoch 335/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565762972.0111 - val_loss: 5749979750.9668\n",
      "Epoch 336/500\n",
      "4332/4332 [==============================] - 3s 585us/step - loss: 8565758250.3121 - val_loss: 5749974797.2251\n",
      "Epoch 337/500\n",
      "4332/4332 [==============================] - 3s 581us/step - loss: 8565753951.9705 - val_loss: 5749970433.8893\n",
      "Epoch 338/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565749553.4035 - val_loss: 5749966259.4834\n",
      "Epoch 339/500\n",
      "4332/4332 [==============================] - 2s 568us/step - loss: 8565744921.5291 - val_loss: 5749961514.5092\n",
      "Epoch 340/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565740634.7701 - val_loss: 5749957576.2657\n",
      "Epoch 341/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565736374.7221 - val_loss: 5749952925.7565\n",
      "Epoch 342/500\n",
      "4332/4332 [==============================] - 2s 576us/step - loss: 8565731803.8338 - val_loss: 5749948633.2694\n",
      "Epoch 343/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8565727571.9151 - val_loss: 5749944549.5498\n",
      "Epoch 344/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8565723068.9861 - val_loss: 5749940064.3542\n",
      "Epoch 345/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8565718644.5356 - val_loss: 5749935673.6236\n",
      "Epoch 346/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565714344.5392 - val_loss: 5749931007.0554\n",
      "Epoch 347/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565710034.6150 - val_loss: 5749926856.2657\n",
      "Epoch 348/500\n",
      "4332/4332 [==============================] - 3s 578us/step - loss: 8565705642.6667 - val_loss: 5749922579.8376\n",
      "Epoch 349/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565701243.6270 - val_loss: 5749918283.5720\n",
      "Epoch 350/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565696691.8855 - val_loss: 5749914132.7823\n",
      "Epoch 351/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565692472.4949 - val_loss: 5749909627.7491\n",
      "Epoch 352/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565688323.0729 - val_loss: 5749905237.0184\n",
      "Epoch 353/500\n",
      "4332/4332 [==============================] - 2s 566us/step - loss: 8565683711.4090 - val_loss: 5749901062.6125\n",
      "Epoch 354/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565679178.2235 - val_loss: 5749896384.7085\n",
      "Epoch 355/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8565674666.7849 - val_loss: 5749892308.5461\n",
      "Epoch 356/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565670442.7849 - val_loss: 5749887874.3616\n",
      "Epoch 357/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565666019.3980 - val_loss: 5749883412.7823\n",
      "Epoch 358/500\n",
      "4332/4332 [==============================] - 2s 561us/step - loss: 8565661720.1108 - val_loss: 5749879376.2952\n",
      "Epoch 359/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565656982.1016 - val_loss: 5749874564.2509\n",
      "Epoch 360/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8565653219.9889 - val_loss: 5749870299.1587\n",
      "Epoch 361/500\n",
      "4332/4332 [==============================] - 2s 553us/step - loss: 8565648536.9381 - val_loss: 5749866046.3469\n",
      "Epoch 362/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565644101.2595 - val_loss: 5749861474.2435\n",
      "Epoch 363/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565640011.1690 - val_loss: 5749857599.2915\n",
      "Epoch 364/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8565635590.1459 - val_loss: 5749852932.7232\n",
      "Epoch 365/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565630644.1219 - val_loss: 5749848372.9004\n",
      "Epoch 366/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565626745.0268 - val_loss: 5749844434.6568\n",
      "Epoch 367/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8565622195.8855 - val_loss: 5749839969.2989\n",
      "Epoch 368/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565617717.4220 - val_loss: 5749835845.9041\n",
      "Epoch 369/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8565613658.5337 - val_loss: 5749831407.9410\n",
      "Epoch 370/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565609071.8079 - val_loss: 5749827091.8376\n",
      "Epoch 371/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565604697.5882 - val_loss: 5749822677.4908\n",
      "Epoch 372/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565600307.0582 - val_loss: 5749818192.2952\n",
      "Epoch 373/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565595596.4691 - val_loss: 5749813966.8782\n",
      "Epoch 374/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565591336.8938 - val_loss: 5749810032.4133\n",
      "Epoch 375/500\n",
      "4332/4332 [==============================] - 2s 571us/step - loss: 8565587295.0249 - val_loss: 5749805074.8930\n",
      "Epoch 376/500\n",
      "4332/4332 [==============================] - 2s 569us/step - loss: 8565582675.3241 - val_loss: 5749801250.9520\n",
      "Epoch 377/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565578103.3721 - val_loss: 5749796600.4428\n",
      "Epoch 378/500\n",
      "4332/4332 [==============================] - 3s 586us/step - loss: 8565573874.9991 - val_loss: 5749792170.0369\n",
      "Epoch 379/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565569313.8024 - val_loss: 5749787968.2362\n",
      "Epoch 380/500\n",
      "4332/4332 [==============================] - 3s 636us/step - loss: 8565565148.7793 - val_loss: 5749783435.8081\n",
      "Epoch 381/500\n",
      "4332/4332 [==============================] - 3s 614us/step - loss: 8565561090.4820 - val_loss: 5749779422.9373\n",
      "Epoch 382/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8565556084.2992 - val_loss: 5749774563.6605\n",
      "Epoch 383/500\n",
      "4332/4332 [==============================] - 3s 668us/step - loss: 8565551899.4201 - val_loss: 5749770345.8007\n",
      "Epoch 384/500\n",
      "4332/4332 [==============================] - 3s 625us/step - loss: 8565547434.4303 - val_loss: 5749766447.2325\n",
      "Epoch 385/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565543159.4903 - val_loss: 5749761540.7232\n",
      "Epoch 386/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8565538853.7027 - val_loss: 5749757713.0037\n",
      "Epoch 387/500\n",
      "4332/4332 [==============================] - 3s 585us/step - loss: 8565534608.9012 - val_loss: 5749753239.1439\n",
      "Epoch 388/500\n",
      "4332/4332 [==============================] - 3s 600us/step - loss: 8565529906.3490 - val_loss: 5749748635.8672\n",
      "Epoch 389/500\n",
      "4332/4332 [==============================] - 3s 669us/step - loss: 8565525779.6196 - val_loss: 5749744646.6125\n",
      "Epoch 390/500\n",
      "4332/4332 [==============================] - 3s 599us/step - loss: 8565521155.7821 - val_loss: 5749739736.3247\n",
      "Epoch 391/500\n",
      "4332/4332 [==============================] - 3s 679us/step - loss: 8565516870.2050 - val_loss: 5749735770.6863\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4332/4332 [==============================] - 3s 653us/step - loss: 8565512585.3370 - val_loss: 5749731572.6642\n",
      "Epoch 393/500\n",
      "4332/4332 [==============================] - 3s 615us/step - loss: 8565508052.9788 - val_loss: 5749727016.6199\n",
      "Epoch 394/500\n",
      "4332/4332 [==============================] - 3s 589us/step - loss: 8565503392.0295 - val_loss: 5749722936.6790\n",
      "Epoch 395/500\n",
      "4332/4332 [==============================] - 3s 602us/step - loss: 8565499267.3093 - val_loss: 5749718219.0996\n",
      "Epoch 396/500\n",
      "4332/4332 [==============================] - 2s 577us/step - loss: 8565494817.0933 - val_loss: 5749714048.4723\n",
      "Epoch 397/500\n",
      "4332/4332 [==============================] - 3s 580us/step - loss: 8565490313.1006 - val_loss: 5749709701.1956\n",
      "Epoch 398/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565485993.2484 - val_loss: 5749705054.4649\n",
      "Epoch 399/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8565481357.1191 - val_loss: 5749701132.2804\n",
      "Epoch 400/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565477297.5217 - val_loss: 5749696461.9336\n",
      "Epoch 401/500\n",
      "4332/4332 [==============================] - 3s 619us/step - loss: 8565472772.4912 - val_loss: 5749692196.8413\n",
      "Epoch 402/500\n",
      "4332/4332 [==============================] - 3s 615us/step - loss: 8565468467.0582 - val_loss: 5749688065.8893\n",
      "Epoch 403/500\n",
      "4332/4332 [==============================] - 3s 626us/step - loss: 8565463894.9880 - val_loss: 5749683600.5314\n",
      "Epoch 404/500\n",
      "4332/4332 [==============================] - 3s 626us/step - loss: 8565459835.6270 - val_loss: 5749679402.5092\n",
      "Epoch 405/500\n",
      "4332/4332 [==============================] - 3s 626us/step - loss: 8565455086.7442 - val_loss: 5749675157.2546\n",
      "Epoch 406/500\n",
      "4332/4332 [==============================] - 3s 615us/step - loss: 8565450898.5559 - val_loss: 5749670439.6753\n",
      "Epoch 407/500\n",
      "4332/4332 [==============================] - 3s 632us/step - loss: 8565446230.5152 - val_loss: 5749666261.4908\n",
      "Epoch 408/500\n",
      "4332/4332 [==============================] - 3s 625us/step - loss: 8565441722.2678 - val_loss: 5749661917.9926\n",
      "Epoch 409/500\n",
      "4332/4332 [==============================] - 3s 628us/step - loss: 8565437798.1163 - val_loss: 5749657393.1218\n",
      "Epoch 410/500\n",
      "4332/4332 [==============================] - 3s 629us/step - loss: 8565433178.4155 - val_loss: 5749653403.8672\n",
      "Epoch 411/500\n",
      "4332/4332 [==============================] - 3s 634us/step - loss: 8565428592.2807 - val_loss: 5749648733.5203\n",
      "Epoch 412/500\n",
      "4332/4332 [==============================] - 3s 642us/step - loss: 8565424622.2715 - val_loss: 5749644669.6384\n",
      "Epoch 413/500\n",
      "4332/4332 [==============================] - 3s 647us/step - loss: 8565419982.5965 - val_loss: 5749639975.6753\n",
      "Epoch 414/500\n",
      "4332/4332 [==============================] - 3s 629us/step - loss: 8565415679.0545 - val_loss: 5749635632.1771\n",
      "Epoch 415/500\n",
      "4332/4332 [==============================] - 3s 658us/step - loss: 8565411654.9141 - val_loss: 5749631556.0148\n",
      "Epoch 416/500\n",
      "4332/4332 [==============================] - 3s 657us/step - loss: 8565406919.5051 - val_loss: 5749626598.4945\n",
      "Epoch 417/500\n",
      "4332/4332 [==============================] - 3s 630us/step - loss: 8565402252.6464 - val_loss: 5749622680.0886\n",
      "Epoch 418/500\n",
      "4332/4332 [==============================] - 3s 638us/step - loss: 8565398372.9344 - val_loss: 5749618191.1144\n",
      "Epoch 419/500\n",
      "4332/4332 [==============================] - 3s 630us/step - loss: 8565393593.5586 - val_loss: 5749613926.0221\n",
      "Epoch 420/500\n",
      "4332/4332 [==============================] - 3s 641us/step - loss: 8565389286.7073 - val_loss: 5749609771.4539\n",
      "Epoch 421/500\n",
      "4332/4332 [==============================] - 3s 631us/step - loss: 8565385035.4054 - val_loss: 5749605030.2583\n",
      "Epoch 422/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8565380421.2595 - val_loss: 5749600852.0738\n",
      "Epoch 423/500\n",
      "4332/4332 [==============================] - 3s 646us/step - loss: 8565375879.0914 - val_loss: 5749596445.2841\n",
      "Epoch 424/500\n",
      "4332/4332 [==============================] - 3s 635us/step - loss: 8565371869.2521 - val_loss: 5749592129.1808\n",
      "Epoch 425/500\n",
      "4332/4332 [==============================] - 3s 630us/step - loss: 8565367118.9511 - val_loss: 5749587923.6015\n",
      "Epoch 426/500\n",
      "4332/4332 [==============================] - 3s 619us/step - loss: 8565362730.3121 - val_loss: 5749583532.8708\n",
      "Epoch 427/500\n",
      "4332/4332 [==============================] - 3s 597us/step - loss: 8565358548.7424 - val_loss: 5749579334.8487\n",
      "Epoch 428/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8565354144.9751 - val_loss: 5749575069.7565\n",
      "Epoch 429/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565349718.6334 - val_loss: 5749570135.8524\n",
      "Epoch 430/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565345702.1754 - val_loss: 5749565934.0517\n",
      "Epoch 431/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565341096.5392 - val_loss: 5749561822.9373\n",
      "Epoch 432/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565336638.4044 - val_loss: 5749557675.9262\n",
      "Epoch 433/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565332160.7682 - val_loss: 5749553423.1144\n",
      "Epoch 434/500\n",
      "4332/4332 [==============================] - 2s 563us/step - loss: 8565327518.6113 - val_loss: 5749548449.5351\n",
      "Epoch 435/500\n",
      "4332/4332 [==============================] - 2s 561us/step - loss: 8565323523.7821 - val_loss: 5749544456.5018\n",
      "Epoch 436/500\n",
      "4332/4332 [==============================] - 3s 579us/step - loss: 8565318849.8319 - val_loss: 5749539738.9225\n",
      "Epoch 437/500\n",
      "4332/4332 [==============================] - 3s 577us/step - loss: 8565314316.2918 - val_loss: 5749535588.1328\n",
      "Epoch 438/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8565310447.9261 - val_loss: 5749531484.5756\n",
      "Epoch 439/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565305488.7830 - val_loss: 5749526605.4613\n",
      "Epoch 440/500\n",
      "4332/4332 [==============================] - 2s 568us/step - loss: 8565301280.8569 - val_loss: 5749522258.1845\n",
      "Epoch 441/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565297267.8264 - val_loss: 5749518056.3838\n",
      "Epoch 442/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565292616.3324 - val_loss: 5749514141.7565\n",
      "Epoch 443/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565288265.2779 - val_loss: 5749509447.7934\n",
      "Epoch 444/500\n",
      "4332/4332 [==============================] - 2s 557us/step - loss: 8565283909.1413 - val_loss: 5749505194.9816\n",
      "Epoch 445/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8565279558.2050 - val_loss: 5749500882.6568\n",
      "Epoch 446/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565275046.4118 - val_loss: 5749496299.2177\n",
      "Epoch 447/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565270667.4644 - val_loss: 5749492128.5904\n",
      "Epoch 448/500\n",
      "4332/4332 [==============================] - 2s 560us/step - loss: 8565266218.3121 - val_loss: 5749487903.1734\n",
      "Epoch 449/500\n",
      "4332/4332 [==============================] - 3s 592us/step - loss: 8565261792.3250 - val_loss: 5749483441.5941\n",
      "Epoch 450/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8565257558.0425 - val_loss: 5749478976.2362\n",
      "Epoch 451/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8565252960.6796 - val_loss: 5749474951.0849\n",
      "Epoch 452/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565248620.7350 - val_loss: 5749470229.7269\n",
      "Epoch 453/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565244170.4007 - val_loss: 5749466382.1697\n",
      "Epoch 454/500\n",
      "4332/4332 [==============================] - 3s 578us/step - loss: 8565239861.8947 - val_loss: 5749461688.2066\n",
      "Epoch 455/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565235532.8236 - val_loss: 5749457415.5572\n",
      "Epoch 456/500\n",
      "4332/4332 [==============================] - 2s 562us/step - loss: 8565231365.4367 - val_loss: 5749452843.4539\n",
      "Epoch 457/500\n",
      "4332/4332 [==============================] - 3s 580us/step - loss: 8565226233.9723 - val_loss: 5749448543.4096\n",
      "Epoch 458/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565222391.4903 - val_loss: 5749444652.3985\n",
      "Epoch 459/500\n",
      "4332/4332 [==============================] - 2s 564us/step - loss: 8565218098.3490 - val_loss: 5749439624.0295\n",
      "Epoch 460/500\n",
      "4332/4332 [==============================] - 2s 558us/step - loss: 8565213395.4423 - val_loss: 5749435618.7159\n",
      "Epoch 461/500\n",
      "4332/4332 [==============================] - 2s 565us/step - loss: 8565209010.7036 - val_loss: 5749431346.0664\n",
      "Epoch 462/500\n",
      "4332/4332 [==============================] - 3s 581us/step - loss: 8565204617.8098 - val_loss: 5749426695.5572\n",
      "Epoch 463/500\n",
      "4332/4332 [==============================] - 2s 570us/step - loss: 8565200105.5439 - val_loss: 5749422726.1402\n",
      "Epoch 464/500\n",
      "4332/4332 [==============================] - 3s 588us/step - loss: 8565195778.6002 - val_loss: 5749418059.5720\n",
      "Epoch 465/500\n",
      "4332/4332 [==============================] - 2s 577us/step - loss: 8565191012.9344 - val_loss: 5749413719.8524\n",
      "Epoch 466/500\n",
      "4332/4332 [==============================] - 3s 584us/step - loss: 8565186988.0849 - val_loss: 5749409399.9705\n",
      "Epoch 467/500\n",
      "4332/4332 [==============================] - 2s 576us/step - loss: 8565182771.0582 - val_loss: 5749405150.9373\n",
      "Epoch 468/500\n",
      "4332/4332 [==============================] - 3s 582us/step - loss: 8565178549.3038 - val_loss: 5749400925.5203\n",
      "Epoch 469/500\n",
      "4332/4332 [==============================] - 2s 567us/step - loss: 8565173799.9483 - val_loss: 5749396042.6273\n",
      "Epoch 470/500\n",
      "4332/4332 [==============================] - 3s 589us/step - loss: 8565169723.9224 - val_loss: 5749391773.7565\n",
      "Epoch 471/500\n",
      "4332/4332 [==============================] - 3s 597us/step - loss: 8565164974.6851 - val_loss: 5749387721.2103\n",
      "Epoch 472/500\n",
      "4332/4332 [==============================] - 3s 580us/step - loss: 8565160667.5974 - val_loss: 5749383066.9225\n",
      "Epoch 473/500\n",
      "4332/4332 [==============================] - 3s 582us/step - loss: 8565156545.5956 - val_loss: 5749379030.4354\n",
      "Epoch 474/500\n",
      "4332/4332 [==============================] - 3s 585us/step - loss: 8565151801.4404 - val_loss: 5749374430.9373\n",
      "Epoch 475/500\n",
      "4332/4332 [==============================] - 3s 578us/step - loss: 8565147515.0360 - val_loss: 5749369996.7528\n",
      "Epoch 476/500\n",
      "4332/4332 [==============================] - 3s 581us/step - loss: 8565143302.2641 - val_loss: 5749365983.8819\n",
      "Epoch 477/500\n",
      "4332/4332 [==============================] - 3s 586us/step - loss: 8565138344.6574 - val_loss: 5749361455.2325\n",
      "Epoch 478/500\n",
      "4332/4332 [==============================] - 3s 583us/step - loss: 8565134192.7535 - val_loss: 5749357021.0480\n",
      "Epoch 479/500\n",
      "4332/4332 [==============================] - 3s 579us/step - loss: 8565129903.6307 - val_loss: 5749352657.7122\n",
      "Epoch 480/500\n",
      "4332/4332 [==============================] - 3s 582us/step - loss: 8565125198.8329 - val_loss: 5749348168.7380\n",
      "Epoch 481/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565120978.1422 - val_loss: 5749344344.7970\n",
      "Epoch 482/500\n",
      "4332/4332 [==============================] - 3s 585us/step - loss: 8565116546.9548 - val_loss: 5749339745.2989\n",
      "Epoch 483/500\n",
      "4332/4332 [==============================] - 2s 574us/step - loss: 8565112237.1487 - val_loss: 5749335570.8930\n",
      "Epoch 484/500\n",
      "4332/4332 [==============================] - 3s 580us/step - loss: 8565108183.5789 - val_loss: 5749331203.7786\n",
      "Epoch 485/500\n",
      "4332/4332 [==============================] - 3s 577us/step - loss: 8565103535.6307 - val_loss: 5749326651.5129\n",
      "Epoch 486/500\n",
      "4332/4332 [==============================] - 3s 604us/step - loss: 8565098955.2872 - val_loss: 5749322426.0959\n",
      "Epoch 487/500\n",
      "4332/4332 [==============================] - 3s 601us/step - loss: 8565094854.3232 - val_loss: 5749317921.0627\n",
      "Epoch 488/500\n",
      "4332/4332 [==============================] - 3s 621us/step - loss: 8565090276.8163 - val_loss: 5749313534.1107\n",
      "Epoch 489/500\n",
      "4332/4332 [==============================] - 3s 601us/step - loss: 8565085519.1874 - val_loss: 5749309422.9963\n",
      "Epoch 490/500\n",
      "4332/4332 [==============================] - 3s 584us/step - loss: 8565081702.3527 - val_loss: 5749304866.9520\n",
      "Epoch 491/500\n",
      "4332/4332 [==============================] - 3s 584us/step - loss: 8565077101.4441 - val_loss: 5749300527.2325\n",
      "Epoch 492/500\n",
      "4332/4332 [==============================] - 3s 591us/step - loss: 8565072619.6713 - val_loss: 5749295789.8155\n",
      "Epoch 493/500\n",
      "4332/4332 [==============================] - 2s 575us/step - loss: 8565068550.3823 - val_loss: 5749291635.2472\n",
      "Epoch 494/500\n",
      "4332/4332 [==============================] - 3s 579us/step - loss: 8565063852.3213 - val_loss: 5749287744.2362\n",
      "Epoch 495/500\n",
      "4332/4332 [==============================] - 2s 572us/step - loss: 8565059915.6417 - val_loss: 5749283137.1808\n",
      "Epoch 496/500\n",
      "4332/4332 [==============================] - 3s 581us/step - loss: 8565055268.1662 - val_loss: 5749278821.0775\n",
      "Epoch 497/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8565050780.2475 - val_loss: 5749274056.2657\n",
      "Epoch 498/500\n",
      "4332/4332 [==============================] - 3s 580us/step - loss: 8565046262.0720 - val_loss: 5749269740.1624\n",
      "Epoch 499/500\n",
      "4332/4332 [==============================] - 2s 573us/step - loss: 8565042071.0471 - val_loss: 5749265963.4539\n",
      "Epoch 500/500\n",
      "4332/4332 [==============================] - 2s 577us/step - loss: 8565037402.7701 - val_loss: 5749261501.8745\n"
     ]
    }
   ],
   "source": [
    "H2 = model1.fit(X_train, y_train, verbose=1, epochs=500, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The MSE value of the training and the validation dataset has steadily decreased, as the values are huge, not visible in graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(H2.history[\"loss\"])\n",
    "plt.plot(H2.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2668/2668 [==============================] - 1s 243us/step\n",
      "6317227014.140929\n",
      "The accuracy of the model: 0.9986640944443055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79480.98564466767"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With the new model we calculate the accuracy of the predicted price value and the actual price value for the test dataset, \n",
    "#and it is around 99.8% accurate.\n",
    "\n",
    "print(model1.evaluate(X_test, y_test))\n",
    "y_pred1 = model1.predict(X_test)\n",
    "print(\"The accuracy of the model:\", np.mean(np.abs(1-y_pred1/y_test)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms2 = sqrt(mean_squared_error(y_test, y_pred1))\n",
    "rms2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
